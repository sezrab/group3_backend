[{"id": "http://arxiv.org/abs/2402.08681v1", "title": "Chain Reaction of Ideas: Can Radioactive Decay Predict Technological\n  Innovation?", "summary": "This work demonstrates the application of a birth-death Markov process,\ninspired by radioactive decay, to capture the dynamics of innovation processes.\nLeveraging the Bass diffusion model, we derive a Gompertz-like function\nexplaining the long-term innovation trends. The validity of our model is\nconfirmed using citation data, Google trends, and a recurrent neural network,\nwhich also reveals short-term fluctuations. Further analysis through an\nautomaton model suggests these fluctuations can arise from the inherent\nstochastic nature of the underlying physics.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.08681v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.08681v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.08680v1", "title": "Mitigating Object Hallucination in Large Vision-Language Models via\n  Classifier-Free Guidance", "summary": "The advancement of Large Vision-Language Models (LVLMs) has increasingly\nhighlighted the critical issue of their tendency to hallucinate non-existing\nobjects in the images. To address this issue, previous works focused on using\nspecially curated datasets or powerful LLMs (e.g., GPT-3.5) to rectify the\noutputs of LVLMs. However, these approaches require either expensive\ntraining/fine-tuning or API access to advanced LLMs to correct the model's\noutput post-generation. In this paper, we tackle this challenge by introducing\na framework called Mitigating hallucinAtion via classifieR-Free guIdaNcE\n(MARINE), which is both training-free and API-free, and can effectively and\nefficiently reduce object hallucinations during the generation process.\nSpecifically, MARINE enriches the visual context of LVLMs by integrating\nexisting open-source vision models, and employs classifier-free guidance to\nincorporate the additional object grounding features to improve the precision\nof LVLMs' generations. Through comprehensive evaluations across $6$ popular\nLVLMs with diverse evaluation metrics, we demonstrate the effectiveness of\nMARINE, which even outperforms existing fine-tuning-based methods. Remarkably,\nit not only reduces hallucinations but also improves the detailedness of LVLMs'\ngenerations, as assessed by GPT-4V.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.08680v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.08680v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.08679v1", "title": "COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability", "summary": "Jailbreaks on Large language models (LLMs) have recently received increasing\nattention. For a comprehensive assessment of LLM safety, it is essential to\nconsider jailbreaks with diverse attributes, such as contextual coherence and\nsentiment/stylistic variations, and hence it is beneficial to study\ncontrollable jailbreaking, i.e. how to enforce control on LLM attacks. In this\npaper, we formally formulate the controllable attack generation problem, and\nbuild a novel connection between this problem and controllable text generation,\na well-explored topic of natural language processing. Based on this connection,\nwe adapt the Energy-based Constrained Decoding with Langevin Dynamics (COLD), a\nstate-of-the-art, highly efficient algorithm in controllable text generation,\nand introduce the COLD-Attack framework which unifies and automates the search\nof adversarial LLM attacks under a variety of control requirements such as\nfluency, stealthiness, sentiment, and left-right-coherence. The controllability\nenabled by COLD-Attack leads to diverse new jailbreak scenarios which not only\ncover the standard setting of generating fluent suffix attacks, but also allow\nus to address new controllable attack settings such as revising a user query\nadversarially with minimal paraphrasing, and inserting stealthy attacks in\ncontext with left-right-coherence. Our extensive experiments on various LLMs\n(Llama-2, Mistral, Vicuna, Guanaco, GPT-3.5) show COLD-Attack's broad\napplicability, strong controllability, high success rate, and attack\ntransferability. Our code is available at\nhttps://github.com/Yu-Fangxu/COLD-Attack.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.08679v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.08679v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.08675v1", "title": "Coarse-Graining in Space versus Time", "summary": "Understanding the structure and dynamics of liquids is pivotal for the study\nof larger spatiotemporal processes, particularly for glass-forming materials at\nlow temperatures. The so-called thermodynamic scaling relation, validated for\nmany molecular systems through experiments, offers an efficient means to\nexplore a vast range of time scales along a one-dimensional phase diagram.\nIsomorph theory provides a theoretical framework for thermodynamic scaling\nbased on strong virial-potential energy correlations, but this approach is most\nsuccessful for simple point particles. In particular, isomorph theory has\nresisted extension to complex molecular liquids due to the existence of\nhigh-frequency intramolecular interactions. To elucidate the microscopic origin\nof density scaling for molecular systems, we employ two distinct approaches for\ncoarse-graining in space or in time. The former eliminates fast degrees of\nfreedom by reducing a molecule to a center-of-mass-level description, while the\nlatter involves temporally averaged fluctuations or correlation functions over\nthe characteristic time scale. We show that both approaches yield a consistent\ndensity scaling coefficient for ortho-terphenyl, which is moreover in agreement\nwith the experimental value. Building upon these findings, we derive the\ndensity scaling relationship exhibiting a single-parameter phase diagram from\nfully atomistic simulations. Our results unravel the microscopic nature\nunderlying thermodynamic scaling and shed light on the role of coarse-graining\nfor assessing the slow fluctuations in molecular systems, ultimately enabling\nthe extension of systematic bottom-up approaches to larger and more complex\nmolecular liquids that are experimentally challenging to probe.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.08675v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.08675v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.08674v1", "title": "Human Curriculum Effects Emerge with In-Context Learning in Neural\n  Networks", "summary": "Human learning is sensitive to rule-like structure and the curriculum of\nexamples used for training. In tasks governed by succinct rules, learning is\nmore robust when related examples are blocked across trials, but in the absence\nof such rules, interleaving is more effective. To date, no neural model has\nsimultaneously captured these seemingly contradictory effects. Here we show\nthat this same tradeoff spontaneously emerges with \"in-context learning\" (ICL)\nboth in neural networks trained with metalearning and in large language models\n(LLMs). ICL is the ability to learn new tasks \"in context\" - without weight\nchanges - via an inner-loop algorithm implemented in activation dynamics.\nExperiments with pretrained LLMs and metalearning transformers show that ICL\nexhibits the blocking advantage demonstrated in humans on a task involving\nrule-like structure, and conversely, that concurrent in-weight learning\nreproduces the interleaving advantage observed in humans on tasks lacking such\nstructure.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.08674v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.08674v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.08670v1", "title": "Rec-GPT4V: Multimodal Recommendation with Large Vision-Language Models", "summary": "The development of large vision-language models (LVLMs) offers the potential\nto address challenges faced by traditional multimodal recommendations thanks to\ntheir proficient understanding of static images and textual dynamics. However,\nthe application of LVLMs in this field is still limited due to the following\ncomplexities: First, LVLMs lack user preference knowledge as they are trained\nfrom vast general datasets. Second, LVLMs suffer setbacks in addressing\nmultiple image dynamics in scenarios involving discrete, noisy, and redundant\nimage sequences. To overcome these issues, we propose the novel reasoning\nscheme named Rec-GPT4V: Visual-Summary Thought (VST) of leveraging large\nvision-language models for multimodal recommendation. We utilize user history\nas in-context user preferences to address the first challenge. Next, we prompt\nLVLMs to generate item image summaries and utilize image comprehension in\nnatural language space combined with item titles to query the user preferences\nover candidate items. We conduct comprehensive experiments across four datasets\nwith three LVLMs: GPT4-V, LLaVa-7b, and LLaVa-13b. The numerical results\nindicate the efficacy of VST.", "author": null, "tags": [["Multimodal Language Processing", 0.25661819891728194], ["Text Segmentation", 0.23314181778591472], ["Pragmatic Analysis", 0.19050257426663889], ["Text Summarization", 0.18200612604020178], ["Language Modeling", 0.17828152656077176]], "link": [{"@href": "http://arxiv.org/abs/2402.08670v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.08670v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.08669v1", "title": "Soliton gas of the integrable Boussinesq equation and its generalised\n  hydrodynamics", "summary": "Generalised hydrodynamics (GHD) is a recent and powerful framework to study\nmany-body integrable systems, quantum or classical, out of equilibrium. It has\nbeen applied to several models, from the delta Bose gas to the XXZ spin chain,\nthe KdV soliton gas and many more. Yet it has only been applied to\n(1+1)-dimensional systems and generalisation to higher dimensions of space is\nnon-trivial. We study the Boussinesq equation which, while generally considered\nto be less physically relevant than the KdV equation, is interesting as a\nstationary reduction of the (boosted) Kadomtsev-Petviashvili (KP) equation, a\nprototypical and universal example of a nonlinear integrable PDE in (2+1)\ndimensions. We follow a heuristic approach inspired by the Thermodynamic Bethe\nAnsatz in order to construct the GHD of the Boussinesq soliton gas. Such\napproach allows for a statistical mechanics interpretation of the Boussinesq\nsoliton gas that comes naturally with the GHD picture. This is to be seen as a\nfirst step in the construction of the KP soliton gas, yielding insight on some\nclasses of solutions from which we may be able to build an intuition on how to\ndevise a more general theory. This also offers another perspective on the\nconstruction of anisotropic bidirectional soliton gases previously introduced\nphenomenologically by Congy et al (2021).", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.08669v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.08669v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.08666v1", "title": "Improving Generalization in Semantic Parsing by Increasing Natural\n  Language Variation", "summary": "Text-to-SQL semantic parsing has made significant progress in recent years,\nwith various models demonstrating impressive performance on the challenging\nSpider benchmark. However, it has also been shown that these models often\nstruggle to generalize even when faced with small perturbations of previously\n(accurately) parsed expressions. This is mainly due to the linguistic form of\nquestions in Spider which are overly specific, unnatural, and display limited\nvariation. In this work, we use data augmentation to enhance the robustness of\ntext-to-SQL parsers against natural language variations. Existing approaches\ngenerate question reformulations either via models trained on Spider or only\nintroduce local changes. In contrast, we leverage the capabilities of large\nlanguage models to generate more realistic and diverse questions. Using only a\nfew prompts, we achieve a two-fold increase in the number of questions in\nSpider. Training on this augmented dataset yields substantial improvements on a\nrange of evaluation sets, including robustness benchmarks and out-of-domain\ndata.", "author": null, "tags": [["Question Answering Systems", 0.23658026883478667], ["Natural Language Generation (NLG)", 0.1694921171555391]], "link": [{"@href": "http://arxiv.org/abs/2402.08666v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.08666v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.08664v1", "title": "Tunable Shape Oscillations of Adaptive Droplets", "summary": "Living materials adapt their shape to signals from the environment, yet the\nimpact of shape changes on signal processing and the associated feedback\ndynamics remain unclear. We derive coarse-grained equations for droplets that\nadjust their interfacial tension in response to signals exchanged at contact\nsurfaces, from the microscopic biophysics of adhesion and signaling. We find\nthat droplet pairs exhibit symmetry-breaking, excitability, and oscillations.\nThe underlying critical points reveal novel mechanisms for physical signal\nprocessing through shape adaptation in soft active materials.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.08664v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.08664v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.08658v1", "title": "The Last JITAI? The Unreasonable Effectiveness of Large Language Models\n  in Issuing Just-in-Time Adaptive Interventions: Fostering Physical Activity\n  in a Prospective Cardiac Rehabilitation Setting", "summary": "We explored the viability of Large Language Models (LLMs) for triggering and\npersonalizing content for Just-in-Time Adaptive Interventions (JITAIs) in\ndigital health. JITAIs are being explored as a key mechanism for sustainable\nbehavior change, adapting interventions to an individual's current context and\nneeds. However, traditional rule-based and machine learning models for JITAI\nimplementation face scalability and reliability limitations, such as lack of\npersonalization, difficulty in managing multi-parametric systems, and issues\nwith data sparsity. To investigate JITAI implementation via LLMs, we tested the\ncontemporary overall performance-leading model 'GPT-4' with examples grounded\nin the use case of fostering heart-healthy physical activity in outpatient\ncardiac rehabilitation. Three personas and five sets of context information per\npersona were used as a basis of triggering and personalizing JITAIs.\nSubsequently, we generated a total of 450 proposed JITAI decisions and message\ncontent, divided equally into JITAIs generated by 10 iterations with GPT-4, a\nbaseline provided by 10 laypersons (LayPs), and a gold standard set by 10\nhealthcare professionals (HCPs). Ratings from 27 LayPs indicated that JITAIs\ngenerated by GPT-4 were superior to those by HCPs and LayPs over all assessed\nscales: i.e., appropriateness, engagement, effectiveness, and professionality.\nThis study indicates that LLMs have significant potential for implementing\nJITAIs as a building block of personalized or \"precision\" health, offering\nscalability, effective personalization based on opportunistically sampled\ninformation, and good acceptability.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.08658v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.08658v1", "@rel": "related", "@type": "application/pdf"}]}]