[{"id": "http://arxiv.org/abs/2402.06627v1", "title": "Feedback Loops With Language Models Drive In-Context Reward Hacking", "summary": "Language models influence the external world: they query APIs that read and\nwrite to web pages, generate content that shapes human behavior, and run system\ncommands as autonomous agents. These interactions form feedback loops: LLM\noutputs affect the world, which in turn affect subsequent LLM outputs. In this\nwork, we show that feedback loops can cause in-context reward hacking (ICRH),\nwhere the LLM at test-time optimizes a (potentially implicit) objective but\ncreates negative side effects in the process. For example, consider an LLM\nagent deployed to increase Twitter engagement; the LLM may retrieve its\nprevious tweets into the context window and make them more controversial,\nincreasing engagement but also toxicity. We identify and study two processes\nthat lead to ICRH: output-refinement and policy-refinement. For these\nprocesses, evaluations on static datasets are insufficient -- they miss the\nfeedback effects and thus cannot capture the most harmful behavior. In\nresponse, we provide three recommendations for evaluation to capture more\ninstances of ICRH. As AI development accelerates, the effects of feedback loops\nwill proliferate, increasing the need to understand their role in shaping LLM\nbehavior.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06627v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06627v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06625v1", "title": "Understanding the Effects of Iterative Prompting on Truthfulness", "summary": "The development of Large Language Models (LLMs) has notably transformed\nnumerous sectors, offering impressive text generation capabilities. Yet, the\nreliability and truthfulness of these models remain pressing concerns. To this\nend, we investigate iterative prompting, a strategy hypothesized to refine LLM\nresponses, assessing its impact on LLM truthfulness, an area which has not been\nthoroughly explored. Our extensive experiments delve into the intricacies of\niterative prompting variants, examining their influence on the accuracy and\ncalibration of model responses. Our findings reveal that naive prompting\nmethods significantly undermine truthfulness, leading to exacerbated\ncalibration errors. In response to these challenges, we introduce several\nprompting variants designed to address the identified issues. These variants\ndemonstrate marked improvements over existing baselines, signaling a promising\ndirection for future research. Our work provides a nuanced understanding of\niterative prompting and introduces novel approaches to enhance the truthfulness\nof LLMs, thereby contributing to the development of more accurate and\ntrustworthy AI systems.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06625v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06625v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06624v1", "title": "Empirically Exploring How Novices Write Software Models in Alloy", "summary": "Writing declarative models has numerous benefits, ranging from automated\nreasoning and correction of design-level properties before systems are built,\nto automated testing and debugging of their implementations after they are\nbuilt. Alloy is a declarative modeling language that is well-suited for\nverifying system designs. A key strength of Alloy is its scenario-finding\ntoolset, the Analyzer, which allows users to explore all valid scenarios that\nadhere to the model's constraints up to a user-provided scope. However, even\nwith visualized scenarios, it is difficult to write correct Alloy models. To\naddress this, a growing body of work explores different techniques for\ndebugging Alloy models. In order to develop and evaluate these techniques in an\neffective manor, this paper presents an empirical study of over 97,000 models\nwritten by novice users trying to learn Alloy. We investigate how users write\nboth correct and incorrect models in order to produce a comprehensive benchmark\nfor future use as well as a series of observations to guide debugging and\neducational efforts for Alloy model development.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06624v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06624v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06622v1", "title": "A two-stage algorithm in evolutionary product unit neural networks for\n  classification", "summary": "This paper presents a procedure to add broader diversity at the beginning of\nthe evolutionary process. It consists of creating two initial populations with\ndifferent parameter settings, evolving them for a small number of generations,\nselecting the best individuals from each population in the same proportion and\ncombining them to constitute a new initial population. At this point the main\nloop of an evolutionary algorithm is applied to the new population. The results\nshow that our proposal considerably improves both the efficiency of previous\nmethodologies and also, significantly, their efficacy in most of the data sets.\nWe have carried out our experimentation on twelve data sets from the UCI\nrepository and two complex real-world problems which differ in their number of\ninstances, features and classes.", "author": null, "tags": [], "link": [{"@title": "doi", "@href": "http://dx.doi.org/10.1016/j.eswa.2010.07.028", "@rel": "related"}, {"@href": "http://arxiv.org/abs/2402.06622v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06622v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06619v1", "title": "Aya Dataset: An Open-Access Collection for Multilingual Instruction\n  Tuning", "summary": "Datasets are foundational to many breakthroughs in modern artificial\nintelligence. Many recent achievements in the space of natural language\nprocessing (NLP) can be attributed to the finetuning of pre-trained models on a\ndiverse set of tasks that enables a large language model (LLM) to respond to\ninstructions. Instruction fine-tuning (IFT) requires specifically constructed\nand annotated datasets. However, existing datasets are almost all in the\nEnglish language. In this work, our primary goal is to bridge the language gap\nby building a human-curated instruction-following dataset spanning 65\nlanguages. We worked with fluent speakers of languages from around the world to\ncollect natural instances of instructions and completions. Furthermore, we\ncreate the most extensive multilingual collection to date, comprising 513\nmillion instances through templating and translating existing datasets across\n114 languages. In total, we contribute four key resources: we develop and\nopen-source the Aya Annotation Platform, the Aya Dataset, the Aya Collection,\nand the Aya Evaluation Suite. The Aya initiative also serves as a valuable case\nstudy in participatory research, involving collaborators from 119 countries. We\nsee this as a valuable framework for future research collaborations that aim to\nbridge gaps in resources.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06619v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06619v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06617v1", "title": "FaBERT: Pre-training BERT on Persian Blogs", "summary": "We introduce FaBERT, a Persian BERT-base model pre-trained on the HmBlogs\ncorpus, encompassing both informal and formal Persian texts. FaBERT is designed\nto excel in traditional Natural Language Understanding (NLU) tasks, addressing\nthe intricacies of diverse sentence structures and linguistic styles prevalent\nin the Persian language. In our comprehensive evaluation of FaBERT on 12\ndatasets in various downstream tasks, encompassing Sentiment Analysis (SA),\nNamed Entity Recognition (NER), Natural Language Inference (NLI), Question\nAnswering (QA), and Question Paraphrasing (QP), it consistently demonstrated\nimproved performance, all achieved within a compact model size. The findings\nhighlight the importance of utilizing diverse and cleaned corpora, such as\nHmBlogs, to enhance the performance of language models like BERT in Persian\nNatural Language Processing (NLP) applications. FaBERT is openly accessible at\nhttps://huggingface.co/sbunlp/fabert", "author": null, "tags": [["Natural Language Understanding (NLU)", 0.19834167904595387], ["Named Entity Recognition (NER)", 0.19074084871192173]], "link": [{"@href": "http://arxiv.org/abs/2402.06617v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06617v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06615v1", "title": "Unified picture of measurement-induced ionization in the transmon", "summary": "Despite the high measurement fidelity that can now be reached, the dispersive\nqubit readout of circuit quantum electrodynamics is plagued by a loss of its\nquantum nondemolition character and a decrease in fidelity with increased\nmeasurement strength. In this work we elucidate the nature of this dynamical\nprocess, which we refer to as transmon ionization. We develop a comprehensive\nframework which provides a unified physical picture of the origin of transmon\nionization. This framework consists of three complementary levels of\ndescriptions: a fully quantized transmon-resonator model, a semiclassical model\nwhere the resonator is treated as a classical drive on the transmon, and a\nfully classical model. Crucially, all three approaches preserve the full cosine\npotential of the transmon, and lead to similar predictions. This framework\nidentifies the multiphoton resonances responsible for transmon ionization. It\nalso allows us to efficiently compute numerical estimates of the photon number\nthreshold for ionization, which are in remarkable agreement with recent\nexperimental results. The set of tools developed within this work are both\nconceptually and computationally simple, and we expect them to become an\nintegral part of the theoretical support of all circuit QED experiments.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06615v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06615v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06611v1", "title": "Image-based Deep Learning for the time-dependent prediction of fresh\n  concrete properties", "summary": "Increasing the degree of digitisation and automation in the concrete\nproduction process can play a crucial role in reducing the CO$_2$ emissions\nthat are associated with the production of concrete. In this paper, a method is\npresented that makes it possible to predict the properties of fresh concrete\nduring the mixing process based on stereoscopic image sequences of the\nconcretes flow behaviour. A Convolutional Neural Network (CNN) is used for the\nprediction, which receives the images supported by information on the mix\ndesign as input. In addition, the network receives temporal information in the\nform of the time difference between the time at which the images are taken and\nthe time at which the reference values of the concretes are carried out. With\nthis temporal information, the network implicitly learns the time-dependent\nbehaviour of the concretes properties. The network predicts the slump flow\ndiameter, the yield stress and the plastic viscosity. The time-dependent\nprediction potentially opens up the pathway to determine the temporal\ndevelopment of the fresh concrete properties already during mixing. This\nprovides a huge advantage for the concrete industry. As a result,\ncountermeasures can be taken in a timely manner. It is shown that an approach\nbased on depth and optical flow images, supported by information of the mix\ndesign, achieves the best results.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06611v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06611v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06609v1", "title": "You Still See Me: How Data Protection Supports the Architecture of ML\n  Surveillance", "summary": "Data forms the backbone of machine learning. Thus, data protection law has\nstrong bearing on how ML systems are governed. Given that most requirements\naccompany the processing of personal data, organizations have an incentive to\nkeep their data out of legal scope. Privacy-preserving techniques incentivized\nby data protection law -- data protection techniques -- constitute an important\nstrategy for ML development because they are used to distill data until it\npotentially falls outside the scope of data protection laws.\n  In this paper, we examine the impact of a rhetoric that deems data wrapped in\nprivacy-preserving techniques as data that is \"good-to-go\". We show how the\napplication of data protection techniques in the development of ML systems --\nfrom private set intersection as part of dataset curation to homomorphic\nencryption and federated learning as part of model computation to the framing\nof the privacy-utility trade-off as part of model updating -- can further\nsupport individual monitoring and data consolidation. With data accumulation at\nthe core of how the ML pipeline is configured, we argue that data protection\ntechniques are often instrumentalized in ways that support infrastructures of\nsurveillance, rather than to protect individuals associated with data. Finally,\nwe propose technology and policy strategies to evaluate data protection\ntechniques in light of the protections they actually confer. We conclude by\nhighlighting the role that security technologists might play in devising\npolicies that combat surveillance ML technologies -- recommending the\nadversarial mindset inherent to the profession to more precisely articulate and\nprevent the use of \"privacy-preserving\" scaffoldings that support surveillance.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06609v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06609v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06608v1", "title": "TIC: Translate-Infer-Compile for accurate 'text to plan' using LLMs and\n  logical intermediate representations", "summary": "We study the problem of generating plans for given natural language planning\ntask requests. On one hand, LLMs excel at natural language processing but do\nnot perform well on planning. On the other hand, classical planning tools excel\nat planning tasks but require input in a structured language such as the\nPlanning Domain Definition Language (PDDL). We leverage the strengths of both\nthe techniques by using an LLM for generating the PDDL representation (task\nPDDL) of planning task requests followed by using a classical planner for\ncomputing a plan. Unlike previous approaches that use LLMs for generating task\nPDDLs directly, our approach comprises of (a) translate: using an LLM only for\ngenerating a logically interpretable intermediate representation of natural\nlanguage task descriptions, (b) infer: deriving additional logically dependent\ninformation from the intermediate representation using a logic reasoner\n(currently, Answer Set Programming solver), and (c) compile: generating the\ntarget task PDDL from the base and inferred information. We observe that using\nan LLM to only output the intermediate representation significantly reduces LLM\nerrors. Consequently, TIC approach achieves, for at least one LLM, high\naccuracy on task PDDL generation for all seven domains of our evaluation\ndataset.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06608v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06608v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06606v1", "title": "RQP-SGD: Differential Private Machine Learning through Noisy SGD and\n  Randomized Quantization", "summary": "The rise of IoT devices has prompted the demand for deploying machine\nlearning at-the-edge with real-time, efficient, and secure data processing. In\nthis context, implementing machine learning (ML) models with real-valued weight\nparameters can prove to be impractical particularly for large models, and there\nis a need to train models with quantized discrete weights. At the same time,\nthese low-dimensional models also need to preserve privacy of the underlying\ndataset. In this work, we present RQP-SGD, a new approach for\nprivacy-preserving quantization to train machine learning models for low-memory\nML-at-the-edge. This approach combines differentially private stochastic\ngradient descent (DP-SGD) with randomized quantization, providing a measurable\nprivacy guarantee in machine learning. In particular, we study the utility\nconvergence of implementing RQP-SGD on ML tasks with convex objectives and\nquantization constraints and demonstrate its efficacy over deterministic\nquantization. Through experiments conducted on two datasets, we show the\npractical effectiveness of RQP-SGD.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06606v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06606v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06599v1", "title": "On the Out-Of-Distribution Generalization of Multimodal Large Language\n  Models", "summary": "We investigate the generalization boundaries of current Multimodal Large\nLanguage Models (MLLMs) via comprehensive evaluation under out-of-distribution\nscenarios and domain-specific tasks. We evaluate their zero-shot generalization\nacross synthetic images, real-world distributional shifts, and specialized\ndatasets like medical and molecular imagery. Empirical results indicate that\nMLLMs struggle with generalization beyond common training domains, limiting\ntheir direct application without adaptation. To understand the cause of\nunreliable performance, we analyze three hypotheses: semantic\nmisinterpretation, visual feature extraction insufficiency, and mapping\ndeficiency. Results identify mapping deficiency as the primary hurdle. To\naddress this problem, we show that in-context learning (ICL) can significantly\nenhance MLLMs' generalization, opening new avenues for overcoming\ngeneralization barriers. We further explore the robustness of ICL under\ndistribution shifts and show its vulnerability to domain shifts, label shifts,\nand spurious correlation shifts between in-context examples and test data.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06599v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06599v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06598v1", "title": "CigaR: Cost-efficient Program Repair with LLMs", "summary": "Large language models (LLM) have proven to be effective at automated program\nrepair (APR). However, using LLMs can be highly costly, with companies\ninvoicing users by the number of tokens. In this paper, we propose CigaR, the\nfirst LLM-based APR tool that focuses on minimizing the repair cost. CigaR\nworks in two major steps: generating a plausible patch and multiplying\nplausible patches. CigaR optimizes the prompts and the prompt setting to\nmaximize the information given to LLMs in the smallest possible number of\ntokens. Our experiments on 267 bugs from the widely used Defects4J dataset\nshows that CigaR reduces the token cost by 62. On average, CigaR spends 171k\ntokens per bug while the baseline uses 451k tokens. On the subset of bugs that\nare fixed by both, CigaR spends 20k per bug while the baseline uses 695k\ntokens, a cost saving of 97. Our extensive experiments show that CigaR is a\ncost-effective LLM-based program repair tool that uses a low number of tokens\nto generate automatic patches.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06598v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06598v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06596v1", "title": "Understanding the Weakness of Large Language Model Agents within a\n  Complex Android Environment", "summary": "Large language models (LLMs) have empowered intelligent agents to execute\nintricate tasks within domain-specific software such as browsers and games.\nHowever, when applied to general-purpose software systems like operating\nsystems, LLM agents face three primary challenges. Firstly, the action space is\nvast and dynamic, posing difficulties for LLM agents to maintain an up-to-date\nunderstanding and deliver accurate responses. Secondly, real-world tasks often\nrequire inter-application cooperation}, demanding farsighted planning from LLM\nagents. Thirdly, agents need to identify optimal solutions aligning with user\nconstraints, such as security concerns and preferences. These challenges\nmotivate AndroidArena, an environment and benchmark designed to evaluate LLM\nagents on a modern operating system. To address high-cost of manpower, we\ndesign a scalable and semi-automated method to construct the benchmark. In the\ntask evaluation, AndroidArena incorporates accurate and adaptive metrics to\naddress the issue of non-unique solutions. Our findings reveal that even\nstate-of-the-art LLM agents struggle in cross-APP scenarios and adhering to\nspecific constraints. Additionally, we identify a lack of four key\ncapabilities, i.e., understanding, reasoning, exploration, and reflection, as\nprimary reasons for the failure of LLM agents. Furthermore, we provide\nempirical analysis on the failure of reflection, and improve the success rate\nby 27% with our proposed exploration strategy. This work is the first to\npresent valuable insights in understanding fine-grained weakness of LLM agents,\nand offers a path forward for future research in this area. Environment,\nbenchmark, and evaluation code for AndroidArena are released at\nhttps://github.com/AndroidArenaAgent/AndroidArena.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06596v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06596v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06595v1", "title": "Damping of density oscillations from bulk viscosity in quark matter", "summary": "We study the damping of density oscillations in the quark matter phase that\nmight occur in compact stars. To this end we compute the bulk viscosity and the\nassociated damping time in three-flavor quark matter, considering both\nnonleptonic and semileptonic electroweak processes. We use two different\nequations of state of quark matter, more precisely, the MIT bag model and\nperturbative QCD, including the leading order corrections in the strong\ncoupling constant. We analyze the dependence of our results on the density,\ntemperature and value of strange quark mass in each case. We then find that the\nmaximum of the bulk viscosity is in the range of temperature from 0.01 to 0.1\nMeV for frequencies around 1 kHz, while the associated minimal damping times of\nthe density oscillations at those temperatures might be in the range of few to\nhundreds milliseconds. Our results suggest that bulk viscous damping might be\nrelevant in the post-merger phase after the collision of two neutron stars if\ndeconfined matter is achieved in the process.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06595v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06595v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06592v1", "title": "Self-consistent context aware conformer transducer for speech\n  recognition", "summary": "We propose a novel neural network architecture based on conformer transducer\nthat adds contextual information flow to the ASR systems. Our method improves\nthe accuracy of recognizing uncommon words while not harming the word error\nrate of regular words. We explore the uncommon words accuracy improvement when\nwe use the new model and/or shallow fusion with context language model. We\nfound that combination of both provides cumulative gain in uncommon words\nrecognition accuracy.", "author": null, "tags": [["Stop Words Removal", 0.34304770969300125], ["Word Embeddings", 0.3309185621050697], ["Lemmatization", 0.2961140827030572], ["Automatic Speech Recognition (ASR)", 0.24005395732556886], ["Speech Recognition", 0.19561234088892676]], "link": [{"@href": "http://arxiv.org/abs/2402.06592v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06592v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06591v1", "title": "Random DFA With One Added Transition", "summary": "Every language recognized by a non-deterministic finite automaton can be\nrecognized by a deterministic automaton, at the cost of a potential increase of\nthe number of states, which in the worst case can go from $n$ states to $2^n$\nstates. In this article, we investigate this classical result in a\nprobabilistic setting where we take a deterministic automaton with $n$ states\nuniformly at random and add just one random transition. These automata are\nalmost deterministic in the sense that only one state has a non-deterministic\nchoice when reading an input letter. In our model, each state has a fixed\nprobability to be final. We prove that for any $d\\geq 1$, with non-negligible\nprobability the minimal (deterministic) automaton of the language recognized by\nsuch an automaton has more than $n^d$ states; as a byproduct, the expected size\nof its minimal automaton grows faster than any polynomial. Our result also\nholds when each state is final with some probability that depends on $n$, as\nlong as it is not too close to $0$ and $1$, at distance at least\n$\\Omega(\\frac1{\\sqrt{n}})$ to be precise, therefore allowing models with a\nsublinear number of final states in expectation.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06591v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06591v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06589v1", "title": "Modular Redesign of Mechatronic Systems: Formulation of Module\n  Specifications Guaranteeing System Dynamics Specifications", "summary": "Complex mechatronic systems are typically composed of interconnected modules,\noften developed by independent teams. This development process challenges the\nverification of system specifications before all modules are integrated. To\naddress this challenge, a modular redesign framework is proposed in this paper.\nHerein, first, allowed changes in the dynamics (represented by frequency\nresponse functions (FRFs)) of the redesigned system are defined with respect to\nthe original system model, which already satisfies system specifications.\nSecond, these allowed changes in the overall system dynamics (or system\nredesign specifications) are automatically translated to dynamics (FRF)\nspecifications on module level that, when satisfied, guarantee overall system\ndynamics (FRF) specifications. This modularity in specification management\nsupports local analysis and verification of module design changes, enabling\ndesign teams to work in parallel without the need to iteratively rebuild the\nsystem model to check fulfilment of system FRF specifications. A modular\nredesign process results that shortens time-to-market and decreases redesign\ncosts. The framework's effectiveness is demonstrated through three examples of\nincreasing complexity, highlighting its potential to enable modular mechatronic\nsystem (re)design.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06589v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06589v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06588v1", "title": "Precision Air Flow Control via EHD Actuator: A Co-simulation and Control\n  Design Case Study", "summary": "A Dielectric Barrier Discharge (DBD) plasma actuator for controlling airflow\nis proposed. It consists of diverging and converging nozzles, two concentric\ncylinders and an actuator mounted in-between the two cylinders. The actuator\nemploys electrohydrodynamic (EHD) body force to induce an air jet within the\nair gap between the two cylinders, effectively creating a suction area while\npassing through the diverging nozzle, due to the Coanda effect. While merging\nwith the air stream inside the inner cylinder, the Coanda jet effectively\nenhances amplification of the airflow. The outflow rate is measured by a\nvelocity sensor at the outlet and controlled by the plasma actuator. The\ncontrol strategy is based on the Active Disturbance Rejection Control (ADRC)\nand compared to the baseline PID controller. The actuator was modelled by\nseamlessly linking two modeling platforms for a co-simulation study. The CFD\nsimulation of the plasma and airflow was carried out in the COMSOL\nmulti-physics commercial software, and the control was implemented in the\nSimulink. The DBD plasma model was based on the two-species model of discharge,\nand the electric body force, calculated from the plasma simulation, was used in\nthe Navier-Stokes equation for the turbulent flow simulation. The plasma-air\nflow system was analyzed using the input (the actuator voltage) and output (the\noutlet flow rate) data for the control design. Finally, the performance of the\nsystem of air flow control device was tested and discussed in the co-simulation\nprocess.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06588v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06588v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06586v1", "title": "Analytical model for the relation between signal bandwidth and spatial\n  resolution in Steered-Response Power Phase Transform (SRP-PHAT) maps", "summary": "An analysis of the relationship between the bandwidth of acoustic signals and\nthe required resolution of steered-response power phase transform (SRP-PHAT)\nmaps used for sound source localization is presented. This relationship does\nnot rely on the far-field assumption, nor does it depend on any specific array\ntopology. The proposed analysis considers the computation of a SRP map as a\nprocess of sampling a set of generalized cross-correlation (GCC) functions,\neach one corresponding to a different microphone pair. From this approach, we\nderive a rule that relates GCC bandwidth with inter-microphone distance,\nresolution of the SRP map, and the potential position of the sound source\nrelative to the array position. This rule is a sufficient condition for an\naliasing-free calculation of the specified SRP-PHAT map. Simulation results\nshow that limiting the bandwidth of the GCC according to such rule leads to\nsignificant reductions in sound source localization errors when sources are not\nin the immediate vicinity of the microphone array. These error reductions are\nmore relevant for coarser resolutions of the SRP map, and they happen in both\nanechoic and reverberant environments.", "author": null, "tags": [], "link": [{"@title": "doi", "@href": "http://dx.doi.org/10.1109/ACCESS.2021.3105650", "@rel": "related"}, {"@href": "http://arxiv.org/abs/2402.06586v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06586v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06584v1", "title": "G-SciEdBERT: A Contextualized LLM for Science Assessment Tasks in German", "summary": "The advancement of natural language processing has paved the way for\nautomated scoring systems in various languages, such as German (e.g., German\nBERT [G-BERT]). Automatically scoring written responses to science questions in\nGerman is a complex task and challenging for standard G-BERT as they lack\ncontextual knowledge in the science domain and may be unaligned with student\nwriting styles. This paper developed a contextualized German Science Education\nBERT (G-SciEdBERT), an innovative large language model tailored for scoring\nGerman-written responses to science tasks. Using G-BERT, we pre-trained\nG-SciEdBERT on a corpus of 50K German written science responses with 5M tokens\nto the Programme for International Student Assessment (PISA) 2015. We\nfine-tuned G-SciEdBERT on 59 assessment items and examined the scoring\naccuracy. We then compared its performance with G-BERT. Our findings reveal a\nsubstantial improvement in scoring accuracy with G-SciEdBERT, demonstrating a\n10% increase of quadratic weighted kappa compared to G-BERT (mean accuracy\ndifference = 0.096, SD = 0.024). These insights underline the significance of\nspecialized language models like G-SciEdBERT, which is trained to enhance the\naccuracy of automated scoring, offering a substantial contribution to the field\nof AI in education.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06584v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06584v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06575v1", "title": "Modeling Microstrip Antenna", "summary": "In this work, a rectangular microstrip antenna with inset is designed,\nsimulated and optimized. In the optimization process the patch is deformed, it\nnew antenna present a amorphous patch. The optimization process was conducted\nwith Genetic Algorithm (GA), S11 parameters was obtained with full wave\nFinite-Differences Time-Domain (FDTD-3D), and the initial configuration\n(design) was obtained with line transmission and cavite method.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06575v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06575v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06574v1", "title": "Prediction of air pollutants PM10 by ARBX(1) processes", "summary": "This work adopts a Banach-valued time series framework for component-wise\nestimation and prediction, from temporal correlated functional data, in\npresence of exogenous variables. The strong-consistency of the proposed\nfunctional estimator and associated plug-in predictor is formulated. The\nsimulation study undertaken illustrates their large-sample size properties. Air\npollutants PM10 curve forecasting, in the Haute-Normandie region (France), is\naddressed by implementation of the functional time series approach presented", "author": null, "tags": [], "link": [{"@title": "doi", "@href": "http://dx.doi.org/10.1007/s00477-019-01712-z", "@rel": "related"}, {"@href": "http://arxiv.org/abs/2402.06574v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06574v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06573v1", "title": "Exploring quantum criticality in a 4D quantum disordered system", "summary": "Phase transitions are prevalent throughout physics, spanning thermal\nphenomena like water boiling to magnetic transitions in solids. They encompass\ncosmological phase transitions in the early universe and the transition into a\nquark-gluon plasma in high-energy collisions. Quantum phase transitions,\nparticularly intriguing, occur at temperatures near absolute zero and are\ndriven by quantum fluctuations rather than thermal ones. The strength of the\nfluctuations is very sensitive to the dimensionality of the physical systems,\nwhich determines the existence and nature of phase transitions. Low-dimensional\nsystems often exhibit suppression of phase transitions, while high-dimensional\nsystems tend to exhibit mean-field-like behavior. The\nlocalization-delocalization Anderson transition stands out among quantum phase\ntransitions, as it is thought to retain its non-mean-field character across all\ndimensions. This work marks the first observation and characterization of the\nAnderson transition in four dimensions using ultracold atoms as a quantum\nsimulator with synthetic dimensions. We characterize the universal dynamics in\nthe vicinity of the phase transition. We measure the critical exponents\ndescribing the scale-invariant properties of the critical dynamics, which are\nshown to obey Wegner's scaling law. Our work is the first experimental\ndemonstration that the Anderson transition is not mean-field in dimension four.", "author": null, "tags": [["Stemming", 0.20156791800476007]], "link": [{"@href": "http://arxiv.org/abs/2402.06573v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06573v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06563v1", "title": "What is Hiding in Medicine's Dark Matter? Learning with Missing Data in\n  Medical Practices", "summary": "Electronic patient records (EPRs) produce a wealth of data but contain\nsignificant missing information. Understanding and handling this missing data\nis an important part of clinical data analysis and if left unaddressed could\nresult in bias in analysis and distortion in critical conclusions. Missing data\nmay be linked to health care professional practice patterns and imputation of\nmissing data can increase the validity of clinical decisions. This study\nfocuses on statistical approaches for understanding and interpreting the\nmissing data and machine learning based clinical data imputation using a single\ncentre's paediatric emergency data and the data from UK's largest clinical\naudit for traumatic injury database (TARN). In the study of 56,961 data points\nrelated to initial vital signs and observations taken on children presenting to\nan Emergency Department, we have shown that missing data are likely to be\nnon-random and how these are linked to health care professional practice\npatterns. We have then examined 79 TARN fields with missing values for 5,791\ntrauma cases. Singular Value Decomposition (SVD) and k-Nearest Neighbour (kNN)\nbased missing data imputation methods are used and imputation results against\nthe original dataset are compared and statistically tested. We have concluded\nthat the 1NN imputer is the best imputation which indicates a usual pattern of\nclinical decision making: find the most similar patients and take their\nattributes as imputation.", "author": null, "tags": [], "link": [{"@title": "doi", "@href": "http://dx.doi.org/10.1109/BigData59044.2023.10386194", "@rel": "related"}, {"@href": "http://arxiv.org/abs/2402.06563v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06563v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06560v1", "title": "Video Annotator: A framework for efficiently building video classifiers\n  using vision-language models and active learning", "summary": "High-quality and consistent annotations are fundamental to the successful\ndevelopment of robust machine learning models. Traditional data annotation\nmethods are resource-intensive and inefficient, often leading to a reliance on\nthird-party annotators who are not the domain experts. Hard samples, which are\nusually the most informative for model training, tend to be difficult to label\naccurately and consistently without business context. These can arise\nunpredictably during the annotation process, requiring a variable number of\niterations and rounds of feedback, leading to unforeseen expenses and time\ncommitments to guarantee quality.\n  We posit that more direct involvement of domain experts, using a\nhuman-in-the-loop system, can resolve many of these practical challenges. We\npropose a novel framework we call Video Annotator (VA) for annotating,\nmanaging, and iterating on video classification datasets. Our approach offers a\nnew paradigm for an end-user-centered model development process, enhancing the\nefficiency, usability, and effectiveness of video classifiers. Uniquely, VA\nallows for a continuous annotation process, seamlessly integrating data\ncollection and model training.\n  We leverage the zero-shot capabilities of vision-language foundation models\ncombined with active learning techniques, and demonstrate that VA enables the\nefficient creation of high-quality models. VA achieves a median 6.8 point\nimprovement in Average Precision relative to the most competitive baseline\nacross a wide-ranging assortment of tasks. We release a dataset with 153k\nlabels across 56 video understanding tasks annotated by three professional\nvideo editors using VA, and also release code to replicate our experiments at:\nhttp://github.com/netflix/videoannotator.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06560v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06560v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06559v1", "title": "Diffusion-ES: Gradient-free Planning with Diffusion for Autonomous\n  Driving and Zero-Shot Instruction Following", "summary": "Diffusion models excel at modeling complex and multimodal trajectory\ndistributions for decision-making and control. Reward-gradient guided denoising\nhas been recently proposed to generate trajectories that maximize both a\ndifferentiable reward function and the likelihood under the data distribution\ncaptured by a diffusion model. Reward-gradient guided denoising requires a\ndifferentiable reward function fitted to both clean and noised samples,\nlimiting its applicability as a general trajectory optimizer. In this paper, we\npropose DiffusionES, a method that combines gradient-free optimization with\ntrajectory denoising to optimize black-box non-differentiable objectives while\nstaying in the data manifold. Diffusion-ES samples trajectories during\nevolutionary search from a diffusion model and scores them using a black-box\nreward function. It mutates high-scoring trajectories using a truncated\ndiffusion process that applies a small number of noising and denoising steps,\nallowing for much more efficient exploration of the solution space. We show\nthat DiffusionES achieves state-of-the-art performance on nuPlan, an\nestablished closed-loop planning benchmark for autonomous driving. Diffusion-ES\noutperforms existing sampling-based planners, reactive deterministic or\ndiffusion-based policies, and reward-gradient guidance. Additionally, we show\nthat unlike prior guidance methods, our method can optimize non-differentiable\nlanguage-shaped reward functions generated by few-shot LLM prompting. When\nguided by a human teacher that issues instructions to follow, our method can\ngenerate novel, highly complex behaviors, such as aggressive lane weaving,\nwhich are not present in the training data. This allows us to solve the hardest\nnuPlan scenarios which are beyond the capabilities of existing trajectory\noptimization methods and driving policies.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06559v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06559v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06558v1", "title": "The Asymptotic Structure of Cosmological Integrals", "summary": "We provide a general analysis of the asymptotic behaviour of perturbative\ncontributions to observables in arbitrary power-law FRW cosmologies,\nindistinctly the Bunch-Davies wavefunction and cosmological correlators. We\nconsider a large class of scalar toy models, including conformally-coupled and\nmassless scalars in arbitrary dimensions, that admits a first principle\ndefinition in terms of (generalised/weighted) cosmological polytopes. The\nperturbative contributions to an observable can be expressed as an integral of\nthe canonical function associated to such polytopes and to weighted graphs. We\nshow how the asymptotic behaviour of these integrals is governed by a special\nclass of nestohedra living in the graph-weight space, both at tree and loop\nlevel. As the singularities of a cosmological process described by a graph can\nbe associated to its subgraphs, we provide a realisation of the nestohedra as a\nsequential truncation of a top-dimensional simplex based on the underlying\ngraph. This allows us to determine all the possible directions -- both in the\ninfrared and in the ultraviolet --, where the integral can diverge as well as\ntheir divergence degree. Both of them are associated to the facets of the\nnestohedra, which are identified by overlapping tubings of the graph: the\nspecific tubing determines the divergent directions while the number of\noverlapping tubings its degree of divergence. This combinatorial formulation\nmakes straightforward the application of sector decomposition for extracting\nboth leading and subleading divergences from the integral, as the sectors in\nwhich the integration domain can be tiled are identified by the collection of\ncompatible facets of the nestohedra, with the latter that can be determined via\nthe graph tubings. Finally, the leading divergence can be interpreted as a\nrestriction of the canonical function of the relevant polytope onto a special\nhyperplane.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06558v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06558v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06557v1", "title": "The Quantified Boolean Bayesian Network: Theory and Experiments with a\n  Logical Graphical Model", "summary": "This paper introduces the Quantified Boolean Bayesian Network (QBBN), which\nprovides a unified view of logical and probabilistic reasoning. The QBBN is\nmeant to address a central problem with the Large Language Model (LLM), which\nhas become extremely popular in Information Retrieval, which is that the LLM\nhallucinates. A Bayesian Network, by construction, cannot hallucinate, because\nit can only return answers that it can explain. We show how a Bayesian Network\nover an unbounded number of boolean variables can be configured to represent\nthe logical reasoning underlying human language. We do this by creating a\nkey-value version of the First-Order Calculus, for which we can prove\nconsistency and completeness. We show that the model is trivially trained over\nfully observed data, but that inference is non-trivial. Exact inference in a\nBayesian Network is intractable (i.e. $\\Omega(2^N)$ for $N$ variables). For\ninference, we investigate the use of Loopy Belief Propagation (LBP), which is\nnot guaranteed to converge, but which has been shown to often converge in\npractice. Our experiments show that LBP indeed does converge very reliably, and\nour analysis shows that a round of LBP takes time $O(N2^n)$, where $N$ bounds\nthe number of variables considered, and $n$ bounds the number of incoming\nconnections to any factor, and further improvements may be possible. Our\nnetwork is specifically designed to alternate between AND and OR gates in a\nBoolean Algebra, which connects more closely to logical reasoning, allowing a\ncompleteness proof for an expanded version of our network, and also allows\ninference to follow specific but adequate pathways, that turn out to be fast.", "author": ["Gregory Coppola"], "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06557v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06557v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06556v1", "title": "Parameter estimation for quantum jump unraveling", "summary": "We consider the estimation of parameters encoded in the measurement record of\na continuously monitored quantum system in the jump unraveling. This unraveling\npicture corresponds to a single-shot scenario, where information is\ncontinuously gathered. Here, it is generally difficult to assess the precision\nof the estimation procedure via the Fisher Information due to intricate\ntemporal correlations and memory effects. In this paper we provide a full set\nof solutions to this problem. First, for multi-channel renewal processes we\nrelate the Fisher Information to an underlying Markov chain and derive a easily\ncomputable expression for it. For non-renewal processes, we introduce a new\nalgorithm that combines two methods: the monitoring operator method for\nmetrology and the Gillespie algorithm which allows for efficient sampling of a\nstochastic form of the Fisher Information along individual quantum\ntrajectories. We show that this stochastic Fisher Information satisfies useful\nproperties related to estimation in the single-shot scenario. Finally, we\nconsider the case where some information is lost in data\ncompression/post-selection, and provide tools for computing the Fisher\nInformation in this case. All scenarios are illustrated with instructive\nexamples from quantum optics and condensed matter.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06556v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06556v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06550v1", "title": "Self Supervised Learning for Improved Calibrationless Radial MRI with\n  NLINV-Net", "summary": "Purpose: To develop a neural network architecture for improved\ncalibrationless reconstruction of radial data when no ground truth is available\nfor training. Methods: NLINV-Net is a model-based neural network architecture\nthat directly estimates images and coil sensitivities from (radial) k-space\ndata via non-linear inversion (NLINV). Combined with a training strategy using\nself-supervision via data undersampling (SSDU), it can be used for imaging\nproblems where no ground truth reconstructions are available. We validated the\nmethod for (1) real-time cardiac imaging and (2) single-shot subspace-based\nquantitative T1 mapping. Furthermore, region-optimized virtual (ROVir) coils\nwere used to suppress artifacts stemming from outside the FoV and to focus the\nk-space based SSDU loss on the region of interest. NLINV-Net based\nreconstructions were compared with conventional NLINV and PI-CS (parallel\nimaging + compressed sensing) reconstruction and the effect of the\nregion-optimized virtual coils and the type of training loss was evaluated\nqualitatively. Results: NLINV-Net based reconstructions contain significantly\nless noise than the NLINV-based counterpart. ROVir coils effectively suppress\nstreakings which are not suppressed by the neural networks while the\nROVir-based focussed loss leads to visually sharper time series for the\nmovement of the myocardial wall in cardiac real-time imaging. For quantitative\nimaging, T1-maps reconstructed using NLINV-Net show similar quality as PI-CS\nreconstructions, but NLINV-Net does not require slice-specific tuning of the\nregularization parameter. Conclusion: NLINV-Net is a versatile tool for\ncalibrationless imaging which can be used in challenging imaging scenarios\nwhere a ground truth is not available.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06550v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06550v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06549v1", "title": "Bryndza at ClimateActivism 2024: Stance, Target and Hate Event Detection\n  via Retrieval-Augmented GPT-4 and LLaMA", "summary": "This study details our approach for the CASE 2024 Shared Task on Climate\nActivism Stance and Hate Event Detection, focusing on Hate Speech Detection,\nHate Speech Target Identification, and Stance Detection as classification\nchallenges. We explored the capability of Large Language Models (LLMs),\nparticularly GPT-4, in zero- or few-shot settings enhanced by retrieval\naugmentation and re-ranking for Tweet classification. Our goal was to determine\nif LLMs could match or surpass traditional methods in this context.\n  We conducted an ablation study with LLaMA for comparison, and our results\nindicate that our models significantly outperformed the baselines, securing\nsecond place in the Target Detection task. The code for our submission is\navailable at https://github.com/NaiveNeuron/bryndza-case-2024", "author": null, "tags": [["Hate Speech Detection", 0.41272647415093333], ["Sarcasm Detection", 0.23558352619340195], ["Paraphrase Detection", 0.22067636999195392], ["Emotion Detection", 0.20673977944856453], ["Fake News Detection", 0.19728247593523537]], "link": [{"@href": "http://arxiv.org/abs/2402.06549v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06549v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06547v1", "title": "Axion Star Explosions and the Reionization History of the Universe", "summary": "Cosmological structure formation simulations of ultralight axion-like dark\nmatter have shown that an axion star forms at the center of every dark matter\nhalo in the Universe. These axion stars would then form in large numbers during\nthe dark ages, $z \\lesssim 70$. Axion stars would represent the densest axion\nenvironments in the Universe, and as such they can trigger collective processes\nthat cannot otherwise occur for axions in vacuum. In particular, even though\nthe lifetime of individual sub-eV axions decaying into a pair of photons is\nmuch larger than the age of the Universe, axion stars can decay into photons on\nvery short time scales due to parametric resonance. In this talk, based on\narXiv:2302.10206 and arXiv:2301.09769, I will discuss the cosmological\nimplications of such decays. We show that massive enough axion stars will decay\ninto a large number of radio photons which will in turn lead to heating and\nionization during the dark ages which is strongly constrained by Planck. As a\nresult, we find that couplings $10^{-14}\\,{\\rm GeV}^{-1} \\lesssim\ng_{a\\gamma\\gamma} \\lesssim 10^{-10}\\,{\\rm GeV}^{-1}$ are excluded by Planck for\n$10^{-14}\\,{\\rm eV}\\lesssim m_a\\lesssim 10^{-8}\\,{\\rm eV}$ within our benchmark\nmodel of axion star abundance. We also highlight that future measurements of\nthe 21 cm line can have sensitivity to couplings at least one order of\nmagnitude smaller.", "author": ["Miguel Escudero"], "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06547v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06547v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06544v1", "title": "Calibrating Long-form Generations from Large Language Models", "summary": "To enhance Large Language Models' (LLMs) reliability, calibration is\nessential -- the model's assessed confidence scores should align with the\nactual likelihood of its responses being correct. However, current confidence\nelicitation methods and calibration metrics typically rely on a binary\ntrue/false assessment of response correctness. This approach does not apply to\nlong-form generation, where an answer can be partially correct. Addressing this\ngap, we introduce a unified calibration framework, in which both the\ncorrectness of the LLMs' responses and their associated confidence levels are\ntreated as distributions across a range of scores. Within this framework, we\ndevelop three metrics to precisely evaluate LLM calibration and further propose\ntwo confidence elicitation methods based on self-consistency and\nself-evaluation. Our experiments, which include long-form QA and summarization\ntasks, demonstrate that larger models don't necessarily guarantee better\ncalibration, that calibration performance is found to be metric-dependent, and\nthat self-consistency methods excel in factoid datasets. We also find that\ncalibration can be enhanced through techniques such as fine-tuning, integrating\nrelevant source documents, scaling the temperature, and combining\nself-consistency with self-evaluation. Lastly, we showcase a practical\napplication of our system: selecting and cascading open-source models and\nChatGPT to optimize correctness given a limited API budget. This research not\nonly challenges existing notions of LLM calibration but also offers practical\nmethodologies for improving trustworthiness in long-form generation.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06544v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06544v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06543v1", "title": "Supernova dust destruction in the magnetized turbulent ISM", "summary": "Dust in the interstellar medium (ISM) is critical to the absorption and\nintensity of emission profiles used widely in astronomical observations, and\nnecessary for star and planet formation. Supernovae (SNe) both produce and\ndestroy ISM dust. In particular the destruction rate is difficult to assess.\nTheory and prior simulations of dust processing by SNe in a uniform ISM predict\nquite high rates of dust destruction, potentially higher than the supernova\ndust production rate in some cases. Here we show simulations of\nsupernova-induced dust processing with realistic ISM dynamics including\nmagnetic field effects and demonstrate how ISM inhomogeneity and magnetic\nfields inhibit dust destruction. Compared to the non-magnetic homogeneous case,\nthe dust mass destroyed within 1 Myr per SNe is reduced by more than a factor\nof two, which can have a great impact on the ISM dust budget.", "author": null, "tags": [], "link": [{"@title": "doi", "@href": "http://dx.doi.org/10.21203/rs.3.rs-2405487/v1", "@rel": "related"}, {"@href": "http://arxiv.org/abs/2402.06543v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06543v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06542v1", "title": "Improving forecasts of precipitation extremes over Northern and Central\n  Italy using machine learning", "summary": "The accurate prediction of intense precipitation events is one of the main\nobjectives of operational weather services. This task is even more relevant\nnowadays, with the rapid progression of global warming which intensifies these\nevents. Numerical weather prediction models have improved continuously over\ntime, providing uncertainty estimation with dynamical ensembles. However,\ndirect precipitation forecasting is still challenging. Greater availability of\nmachine learning tools paves the way to a hybrid forecasting approach, with the\noptimal combination of physical models, event statistics, and user-oriented\npost-processing. Here we describe a specific chain, based on a random forest\npipeline, specialised in recognizing favourable synoptic conditions leading to\nprecipitation extremes and subsequently classifying extremes into predefined\ntypes. The application focuses on Northern and Central Italy, taken as a\ntestbed region, but is seamlessly extensible to other regions and timescales.\nThe system is called MaLCoX (Machine Learning model predicting Conditions for\neXtreme precipitation) and is running daily at the Italian regional weather\nservice of ARPAE Emilia-Romagna. MalCoX has been trained with the ARCIS gridded\nhigh-resolution precipitation dataset as the target truth, using the last 20\nyears of the ECMWF re-forecast dataset as input predictors. We show that, with\na long enough training period, the optimal blend of larger-scale information\nwith direct model output improves the probabilistic forecast accuracy of\nextremes in the medium range. In addition, with specific methods, we provide a\nuseful diagnostic to convey to forecasters the underlying physical storyline\nwhich makes a meteorological event extreme.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06542v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06542v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06539v1", "title": "Hybridnet for depth estimation and semantic segmentation", "summary": "Semantic segmentation and depth estimation are two important tasks in the\narea of image processing. Traditionally, these two tasks are addressed in an\nindependent manner. However, for those applications where geometric and\nsemantic information is required, such as robotics or autonomous\nnavigation,depth or semantic segmentation alone are not sufficient. In this\npaper, depth estimation and semantic segmentation are addressed together from a\nsingle input image through a hybrid convolutional network. Different from the\nstate of the art methods where features are extracted by a sole feature\nextraction network for both tasks, the proposed HybridNet improves the features\nextraction by separating the relevant features for one task from those which\nare relevant for both. Experimental results demonstrate that HybridNet results\nare comparable with the state of the art methods, as well as the single task\nmethods that HybridNet is based on.", "author": null, "tags": [["Text Segmentation", 0.22584385901823673], ["Semantic Analysis", 0.22540018333634895], ["Information Extraction", 0.1760479703390651]], "link": [{"@title": "doi", "@href": "http://dx.doi.org/10.1109/ICASSP.2018.8462433", "@rel": "related"}, {"@href": "http://arxiv.org/abs/2402.06539v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06539v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06538v1", "title": "An Exercise in Tournament Design: When Some Matches Must Be Scheduled", "summary": "Single-elimination (SE) tournaments are a popular format used in competitive\nenvironments and decision making. Algorithms for SE tournament manipulation\nhave been an active topic of research in recent years. In this paper, we\ninitiate the algorithmic study of a novel variant of SE tournament manipulation\nthat aims to model the fact that certain matchups are highly desired in a\nsporting context, incentivizing an organizer to manipulate the bracket to make\nsuch matchups take place. We obtain both hardness and tractability results. We\nshow that while the problem of computing a bracket enforcing a given set of\nmatches in an SE tournament is NP-hard, there are natural restrictions that\nlead to polynomial-time solvability. In particular, we show polynomial-time\nsolvability if there is a linear ordering on the ability of players with only a\nconstant number of exceptions where a player with lower ability beats a player\nwith higher ability.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06538v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06538v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06536v1", "title": "Relative frequencies of constrained events in stochastic processes: An\n  analytical approach", "summary": "The stochastic simulation algorithm (SSA) and the corresponding Monte Carlo\n(MC) method are among the most common approaches for studying stochastic\nprocesses. They rely on knowledge of interevent probability density functions\n(PDFs) and on information about dependencies between all possible events.\nAnalytical representations of a PDF are difficult to specify in advance, in\nmany real life applications. Knowing the shapes of PDFs, and using experimental\ndata, different optimization schemes can be applied in order to evaluate\nprobability density functions and, therefore, the properties of the studied\nsystem. Such methods, however, are computationally demanding, and often not\nfeasible. We show that, in the case where experimentally accessed properties\nare directly related to the frequencies of events involved, it may be possible\nto replace the heavy Monte Carlo core of optimization schemes with an\nanalytical solution. Such a replacement not only provides a more accurate\nestimation of the properties of the process, but also reduces the simulation\ntime by a factor of order of the sample size (at least $\\approx 10^4$). The\nproposed analytical approach is valid for any choice of PDF. The accuracy,\ncomputational efficiency, and advantages of the method over MC procedures are\ndemonstrated in the exactly solvable case and in the evaluation of branching\nfractions in controlled radical polymerization (CRP) of acrylic monomers. This\npolymerization can be modeled by a constrained stochastic process. Constrained\nsystems are quite common, and this makes the method useful for various\napplications.", "author": null, "tags": [], "link": [{"@title": "doi", "@href": "http://dx.doi.org/10.1103/PhysRevE.92.043306", "@rel": "related"}, {"@href": "http://arxiv.org/abs/2402.06536v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06536v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06534v1", "title": "Control of autoresonant plasma beat-wave wakefield excitation", "summary": "Autoresonant phase-locking of the plasma wakefield to the beat frequency of\ntwo driving lasers offers advantages over conventional wakefield acceleration\nmethods, since it requires less demanding laser parameters and is robust to\nvariations in the target plasma density. Here, we investigate the kinetic and\nnonlinear processes that come into play during autoresonant plasma beat-wave\nacceleration of electrons, their impact on the field amplitude of the\naccelerating structure, and on acceleration efficiency. Particle-in-Cell\nsimulations show that the process depends on the plasma density in a\nnon-trivial way but can be reliably modeled under specific conditions. Beside\nrecovering previous fluid results in the deeply underdense plasma limit, we\ndemonstrate that robust field excitation can be achieved within a fully kinetic\nself-consistent modeling. By adjusting the laser properties, we can amplify the\nelectric field to the desired level, up to wave-breaking, and efficiently\naccelerate particles; we provide suggestions for optimized laser and plasma\nparameters. This versatile and efficient acceleration scheme, producing\nelectrons from tens to hundreds of MeV energies, holds promise for a wide range\nof applications in research industry and medicine.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06534v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06534v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06530v1", "title": "Refining Myocardial Infarction Detection: A Novel Multi-Modal Composite\n  Kernel Strategy in One-Class Classification", "summary": "Early detection of myocardial infarction (MI), a critical condition arising\nfrom coronary artery disease (CAD), is vital to prevent further myocardial\ndamage. This study introduces a novel method for early MI detection using a\none-class classification (OCC) algorithm in echocardiography. Our study\novercomes the challenge of limited echocardiography data availability by\nadopting a novel approach based on Multi-modal Subspace Support Vector Data\nDescription. The proposed technique involves a specialized MI detection\nframework employing multi-view echocardiography incorporating a composite\nkernel in the non-linear projection trick, fusing Gaussian and Laplacian\nsigmoid functions. Additionally, we enhance the update strategy of the\nprojection matrices by adapting maximization for both or one of the modalities\nin the optimization process. Our method boosts MI detection capability by\nefficiently transforming features extracted from echocardiography data into an\noptimized lower-dimensional subspace. The OCC model trained specifically on\ntarget class instances from the comprehensive HMC-QU dataset that includes\nmultiple echocardiography views indicates a marked improvement in MI detection\naccuracy. Our findings reveal that our proposed multi-view approach achieves a\ngeometric mean of 71.24\\%, signifying a substantial advancement in\nechocardiography-based MI diagnosis and offering more precise and efficient\ndiagnostic tools.", "author": null, "tags": [["Paraphrase Detection", 0.19320396796723863], ["Fake News Detection", 0.17452561640788158], ["Sarcasm Detection", 0.17071401909419803]], "link": [{"@href": "http://arxiv.org/abs/2402.06530v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06530v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06529v1", "title": "Introspective Planning: Guiding Language-Enabled Agents to Refine Their\n  Own Uncertainty", "summary": "Large language models (LLMs) exhibit advanced reasoning skills, enabling\nrobots to comprehend natural language instructions and strategically plan\nhigh-level actions through proper grounding. However, LLM hallucination may\nresult in robots confidently executing plans that are misaligned with user\ngoals or, in extreme cases, unsafe. Additionally, inherent ambiguity in natural\nlanguage instructions can induce task uncertainty, particularly in situations\nwhere multiple valid options exist. To address this issue, LLMs must identify\nsuch uncertainty and proactively seek clarification. This paper explores the\nconcept of introspective planning as a systematic method for guiding LLMs in\nforming uncertainty--aware plans for robotic task execution without the need\nfor fine-tuning. We investigate uncertainty quantification in task-level robot\nplanning and demonstrate that introspection significantly improves both success\nrates and safety compared to state-of-the-art LLM-based planning approaches.\nFurthermore, we assess the effectiveness of introspective planning in\nconjunction with conformal prediction, revealing that this combination yields\ntighter confidence bounds, thereby maintaining statistical success guarantees\nwith fewer superfluous user clarification queries.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06529v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06529v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06528v1", "title": "Collider sensitivity to SMEFT heavy-quark operators at one-loop in\n  top-quark processes", "summary": "We study the effects of four-heavy-quark operators in the production of top\nquarks in the framework of the Standard Model Effective Field Theory (SMEFT) at\nthe LHC. In particular, we compute for the first time the total contribution of\nthe four-top-quark operator which enters only at the one-loop level in the\ntop-quark pair production process. Analytical results at one-loop are presented\nfor the gluon- and quark-initiated sub-processes, which allowed a first\ncomplete validation of the SMEFT@NLO in Madgraph5_aMC@NLO. The 95% CL bounds on\nfour-heavy-quark operators from the available top-quark pair and four-top-quark\nproduction data are provided, which are complementary to other bounds found in\nthe literature. We focus on the comparison of the sensitivities of the\ntop-quark pair and the four-top-quark production processes, where in the latter\ncase the four-top-quark operator contributes at tree-level. We conclude that\nthe sensitivities of the two processes to four-heavy-quark operators are\ncomparable. The projected sensitivities of both processes at HL-LHC are also\npresented.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06528v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06528v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06525v1", "title": "Flexible infinite-width graph convolutional networks and the importance\n  of representation learning", "summary": "A common theoretical approach to understanding neural networks is to take an\ninfinite-width limit, at which point the outputs become Gaussian process (GP)\ndistributed. This is known as a neural network Gaussian process (NNGP).\nHowever, the NNGP kernel is fixed, and tunable only through a small number of\nhyperparameters, eliminating any possibility of representation learning. This\ncontrasts with finite-width NNs, which are often believed to perform well\nprecisely because they are able to learn representations. Thus in simplifying\nNNs to make them theoretically tractable, NNGPs may eliminate precisely what\nmakes them work well (representation learning). This motivated us to understand\nwhether representation learning is necessary in a range of graph classification\ntasks. We develop a precise tool for this task, the graph convolutional deep\nkernel machine. This is very similar to an NNGP, in that it is an infinite\nwidth limit and uses kernels, but comes with a `knob' to control the amount of\nrepresentation learning. We found that representation learning is necessary (in\nthe sense that it gives dramatic performance improvements) in graph\nclassification tasks and heterophilous node classification tasks, but not in\nhomophilous node classification tasks.", "author": null, "tags": [["Text Classification", 0.21729236539312102], ["Semantic Analysis", 0.17734115400337666], ["Information Extraction", 0.16307001325629486]], "link": [{"@href": "http://arxiv.org/abs/2402.06525v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06525v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06524v1", "title": "Isochrone fitting of Galactic globular clusters -- V. NGC6397 and\n  NGC6809 (M55)", "summary": "We fit various colour-magnitude diagrams (CMDs) of the Galactic globular\nclusters NGC\\,6397 and NGC\\,6809 (M55) by isochrones from the Dartmouth Stellar\nEvolution Database (DSED) and Bag of Stellar Tracks and Isochrones (BaSTI) for\n$\\alpha$-enhanced [$\\alpha$/Fe]$=+0.4$. For the CMDs, we use data sets from\n{\\it HST}, {\\it Gaia}, VISTA, and other sources utilizing 32 and 23 photometric\nfilters for NGC\\,6397 and NGC\\,6809, respectively, from the ultraviolet to\nmid-infrared. We obtain the following characteristics for NGC\\,6397 and\nNGC\\,6809, respectively: metallicities [Fe/H]$=-1.84\\pm0.02\\pm0.1$ and\n$-1.78\\pm0.02\\pm0.1$ (statistic and systematic uncertainties); distances\n$2.45\\pm0.02\\pm0.06$ and $5.24\\pm0.02\\pm0.18$ kpc; ages $12.9\\pm0.1\\pm0.8$ and\n$13.0\\pm0.1\\pm0.8$ Gyr; reddenings $E(B-V)=0.178\\pm0.006\\pm0.01$ and\n$0.118\\pm0.004\\pm0.01$ mag; extinctions $A_\\mathrm{V}=0.59\\pm0.01\\pm0.02$ and\n$0.37\\pm0.01\\pm0.04$ mag; extinction-to-reddening ratio\n$R_\\mathrm{V}=3.32^{+0.32}_{-0.28}$ and $3.16^{+0.66}_{-0.56}$. Our estimates\nagree with most estimates from the literature. BaSTI gives systematically\nhigher [Fe/H] and lower reddenings than DSED. Despite nearly the same\nmetallicity, age, and helium enrichment, these clusters show a considerable\nhorizontal branch (HB) morphology difference, which must therefore be described\nby another parameter. This parameter must predominantly explain why the least\nmassive HB stars (0.58-0.63 solar masses) are only found within NGC 6809.\nProbably they have been lost by the core-collapse cluster NGC\\,6397 during its\ndynamical evolution and mass segregation. In contrast, NGC\\,6809 has a very low\ncentral concentration and, hence, did not undergo this process.", "author": null, "tags": [], "link": [{"@title": "doi", "@href": "http://dx.doi.org/10.1093/mnras/stad3134", "@rel": "related"}, {"@href": "http://arxiv.org/abs/2402.06524v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06524v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06523v1", "title": "Non-perturbative Studies of Non-conformal Field Theories", "summary": "Many of the exciting features of the Standard Model of the elementary\nparticles are inherently non-perturbative. A theoretical understanding of many\nphysics aspects beyond the Standard Model of elementary particles also requires\na non-perturbative framework. One such framework involves discretizing quantum\nfield theories on a spacetime lattice. We can use this lattice regularization\nmethod to study supersymmetric versions of physics beyond the Standard Model.\nIn this thesis, we discuss the spacetime lattice setup, and with the examples\nof different models, we will see the numerical capability of this tool in\nexploring field theory regimes that are not accessible through perturbation\ntheory. We use an efficient version of the Monte Carlo algorithm to update the\nfield configurations in the path integral and eventually reach the equilibrium\nconfigurations. A version of the gauge/gravity conjecture connects weakly\ncoupled gravitational theories to strongly coupled field theories. We will\nmainly focus on the non-conformal analogs of the conjecture in lower\ndimensions. This thesis mainly discusses the numerical simulation results of\ntwo lower-dimensional models. One is the bosonic version of the BMN matrix\nquantum mechanics and the other is a two-dimensional Yang-Mills theory\ncontaining four supersymmetries. Our numerical results suggest that the phase\ndiagram of bosonic BMN model smoothly interpolates between the bosonic BFSS and\nthe gauged Gaussian model, with first-order deconfinement phase transition at\nall couplings. Our simulation results for two-dimensional Yang-Mills theory\nthat contains four supersymmetries show that this model admits a deconfinement\nphase transition in the limit of a large number of colors. We also show that\nthe nature of the transition looks similar to its maximally supersymmetric\ncousin in the weak coupling regime.", "author": ["Navdeep Singh Dhindsa"], "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06523v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06523v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06520v1", "title": "Accelerating Innovation in 6G Research: Real-Time Capable SDR System\n  Architecture for Rapid Prototyping", "summary": "The next global mobile communication standard 6G strives to push the\ntechnological limits of radio frequency (RF) communication even further than\nits predecessors: Data rates beyond 100 Gbit/s, RF bandwidths above 1 GHz, and\nsub-millisecond latency necessitate very high performance development tools to\nenable the extent of innovation required for 6G's likely features. We propose a\nnew SDR firmware and software architecture designed explicitly to meet these\nchallenging requirements. It relies on Ethernet and commercial off-the-shelf\nnetwork and server components to maximize flexibility and to reduce costs. We\nanalyze state-of-the-art solutions (USRP X440 and other RFSoC-based systems),\nderive architectural design goals, explain resulting design decision in detail,\nand exemplify our architecture's implementation on the XCZU48DR RFSoC. Finally,\nwe prove its performance via measurements and outline how the architecture\nsurpasses the state-of-the-art with respect to sustained RF recording while\nmaintaining high Ethernet bandwidth efficiency. Building a micro-Doppler radar\nexample, we demonstrate its real-time and rapid application development\ncapabilities.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06520v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06520v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06514v1", "title": "Spatial calibration of high-density absorption imaging", "summary": "The accurate determination of atom numbers is an ubiquitous problem in the\nfield of ultracold atoms. For modest atom numbers, absolute calibration\ntechniques are available, however, for large numbers and high densities, the\navailable techniques neglect many-body scattering processes. Here, a spatial\ncalibration technique for time-of-flight absorption images of ultracold atomic\nclouds is presented. The calibration is obtained from radially averaged\nabsorption images and we provide a practical guide to the calibration process.\nIt is shown that the calibration coefficient scales linearly with optical\ndensity and depends on the absorbed photon number for the experimental\nconditions explored here. This allows for the direct inclusion of a spatially\ndependent calibration in the image analysis. For typical ultracold atom clouds\nthe spatial calibration technique leads to corrections in the detected atom\nnumber up to $\\approx\\! 12\\,\\%$ and temperature up to $\\approx \\!14\\,\\%$ in\ncomparison to previous calibration techniques. The technique presented here\naddresses a major difficulty in absorption imaging of ultracold atomic clouds\nand prompts further theoretical work to understand the scattering processes in\nultracold dense clouds of atoms for accurate atom number calibration.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06514v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06514v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06513v1", "title": "Long-lived collective Rydberg excitations in atomic gas achieved via\n  ac-Stark lattice modulation", "summary": "Collective Rydberg excitations provide promising applications ranging from\nquantum information processing, and quantum computing to ultra-sensitive\nelectrometry. However, their short lifetime is an immense obstacle in real-life\nscenarios. The state-of-the-art methods of prolonging the lifetime were only\nimplemented for ground-state quantum memories and would require a redesign to\neffectively work on different atomic transitions. We propose a protocol for\nextending the Rydberg excitation lifetime, which in principle can freeze the\nspin-wave and completely cancel the effects of thermal dephasing. The protocol\nemploys off-resonant ac-Stark lattice modulation of spin waves by interfering\ntwo laser beams on the atomic medium. Our implementation showed that the\nexcitation lifetime can be extended by an order of magnitude, paving the way\ntowards more complex protocols for collective Rydberg excitations.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06513v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06513v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06512v1", "title": "Multimodal Clinical Trial Outcome Prediction with Large Language Models", "summary": "The clinical trial is a pivotal and costly process, often spanning multiple\nyears and requiring substantial financial resources. Therefore, the development\nof clinical trial outcome prediction models aims to exclude drugs likely to\nfail and holds the potential for significant cost savings. Recent data-driven\nattempts leverage deep learning methods to integrate multimodal data for\npredicting clinical trial outcomes. However, these approaches rely on manually\ndesigned modal-specific encoders, which limits both the extensibility to adapt\nnew modalities and the ability to discern similar information patterns across\ndifferent modalities. To address these issues, we propose a multimodal\nmixture-of-experts (LIFTED) approach for clinical trial outcome prediction.\nSpecifically, LIFTED unifies different modality data by transforming them into\nnatural language descriptions. Then, LIFTED constructs unified noise-resilient\nencoders to extract information from modal-specific language descriptions.\nSubsequently, a sparse Mixture-of-Experts framework is employed to further\nrefine the representations, enabling LIFTED to identify similar information\npatterns across different modalities and extract more consistent\nrepresentations from those patterns using the same expert model. Finally, a\nmixture-of-experts module is further employed to dynamically integrate\ndifferent modality representations for prediction, which gives LIFTED the\nability to automatically weigh different modalities and pay more attention to\ncritical information. The experiments demonstrate that LIFTED significantly\nenhances performance in predicting clinical trial outcomes across all three\nphases compared to the best baseline, showcasing the effectiveness of our\nproposed key components.", "author": null, "tags": [["Language Identification", 0.18507067027664556], ["Multimodal Language Processing", 0.1655607704323986]], "link": [{"@href": "http://arxiv.org/abs/2402.06512v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06512v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06509v1", "title": "Asking the Right Question at the Right Time: Human and Model Uncertainty\n  Guidance to Ask Clarification Questions", "summary": "Clarification questions are an essential dialogue tool to signal\nmisunderstanding, ambiguities, and under-specification in language use. While\nhumans are able to resolve uncertainty by asking questions since childhood,\nmodern dialogue systems struggle to generate effective questions. To make\nprogress in this direction, in this work we take a collaborative dialogue task\nas a testbed and study how model uncertainty relates to human uncertainty -- an\nas yet under-explored problem. We show that model uncertainty does not mirror\nhuman clarification-seeking behavior, which suggests that using human\nclarification questions as supervision for deciding when to ask may not be the\nmost effective way to resolve model uncertainty. To address this issue, we\npropose an approach to generating clarification questions based on model\nuncertainty estimation, compare it to several alternatives, and show that it\nleads to significant improvements in terms of task success. Our findings\nhighlight the importance of equipping dialogue systems with the ability to\nassess their own uncertainty and exploit in interaction.", "author": null, "tags": [["Dialogue Systems", 0.21400635409448007]], "link": [{"@href": "http://arxiv.org/abs/2402.06509v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06509v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06505v1", "title": "The role of mobility in epidemics near criticality", "summary": "The general epidemic process (GEP), also known as\nsusceptible-infected-recovered model (SIR), describes how an epidemic spreads\nwithin a population of susceptible individuals who acquire permanent\nimmunization upon recovery. This model exhibits a second-order absorbing state\nphase transition, commonly studied assuming immobile healthy individuals. We\ninvestigate the impact of mobility on disease spreading near the extinction\nthreshold by introducing two generalizations of GEP, where the mobility of\nsusceptible and recovered individuals is examined independently. In both cases,\nincluding mobility violates GEP's rapidity reversal symmetry and alters the\nnumber of absorbing states. The critical dynamics of the models are analyzed\nthrough a perturbative renormalization group approach and large-scale\nstochastic simulations using a Gillespie algorithm. The renormalization group\nanalysis predicts both models to belong to the same novel universality class\ndescribing the critical dynamics of epidemic spreading when the infected\nindividuals interact with a diffusive species and gain immunization upon\nrecovery. At the associated renormalization group fixed point, the immobile\nspecies decouples from the dynamics of the infected species, dominated by the\ncoupling with the diffusive species. Numerical simulations in two dimensions\naffirm our renormalization group results by identifying the same set of\ncritical exponents for both models. Violation of the rapidity reversal symmetry\nis confirmed by breaking the associated hyperscaling relation. Our study\nunderscores the significance of mobility in shaping population spreading\ndynamics near the extinction threshold.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06505v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06505v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06504v1", "title": "Solving Complex Multi-UAV Mission Planning Problems using\n  Multi-objective Genetic Algorithms", "summary": "Due to recent booming of UAVs technologies, these are being used in many\nfields involving complex tasks. Some of them involve a high risk to the vehicle\ndriver, such as fire monitoring and rescue tasks, which make UAVs excellent for\navoiding human risks. Mission Planning for UAVs is the process of planning the\nlocations and actions (loading/dropping a load, taking videos/pictures,\nacquiring information) for the vehicles, typically over a time period. These\nvehicles are controlled from Ground Control Stations (GCSs) where human\noperators use rudimentary systems. This paper presents a new Multi-Objective\nGenetic Algorithm for solving complex Mission Planning Problems (MPP) involving\na team of UAVs and a set of GCSs. A hybrid fitness function has been designed\nusing a Constraint Satisfaction Problem (CSP) to check if solutions are valid\nand Pareto-based measures to look for optimal solutions. The algorithm has been\ntested on several datasets optimizing different variables of the mission, such\nas the makespan, the fuel consumption, distance, etc. Experimental results show\nthat the new algorithm is able to obtain good solutions, however as the problem\nbecomes more complex, the optimal solutions also become harder to find.", "author": null, "tags": [], "link": [{"@title": "doi", "@href": "http://dx.doi.org/10.1007/s00500-016-2376-7", "@rel": "related"}, {"@href": "http://arxiv.org/abs/2402.06504v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06504v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06502v1", "title": "Continuation of Periodic Orbits in Conservative Hybrid Dynamical Systems\n  and its Application to Mechanical Systems with Impulsive Dynamics", "summary": "In autonomous differential equations where a single first integral is\npresent, periodic orbits are well-known to belong to one-parameter families,\nparameterized by the first integral's values. This paper shows that this\ncharacteristic extends to a broader class of conservative hybrid dynamical\nsystems (cHDSs). We define recurrent cHDSs to study periodic orbits,\nintroducing the concept of a hybrid first integral to characterize conservation\nin these systems. Additionally, our work presents a methodology that utilizes\nnumerical continuation methods to generate these periodic orbits, building upon\nthe concept of normal periodic orbits. We specifically compare state-based and\ntime-based implementations of an cHDS as an important application detail in\ngenerating periodic orbits. Furthermore, we showcase the continuation process\nusing exemplary conservative mechanical systems with impulsive dynamics.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06502v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06502v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06501v1", "title": "Scalable Interactive Machine Learning for Future Command and Control", "summary": "Future warfare will require Command and Control (C2) personnel to make\ndecisions at shrinking timescales in complex and potentially ill-defined\nsituations. Given the need for robust decision-making processes and\ndecision-support tools, integration of artificial and human intelligence holds\nthe potential to revolutionize the C2 operations process to ensure adaptability\nand efficiency in rapidly changing operational environments. We propose to\nleverage recent promising breakthroughs in interactive machine learning, in\nwhich humans can cooperate with machine learning algorithms to guide machine\nlearning algorithm behavior. This paper identifies several gaps in\nstate-of-the-art science and technology that future work should address to\nextend these approaches to function in complex C2 contexts. In particular, we\ndescribe three research focus areas that together, aim to enable scalable\ninteractive machine learning (SIML): 1) developing human-AI interaction\nalgorithms to enable planning in complex, dynamic situations; 2) fostering\nresilient human-AI teams through optimizing roles, configurations, and trust;\nand 3) scaling algorithms and human-AI teams for flexibility across a range of\npotential contexts and situations.", "author": null, "tags": [["Stemming", 0.18068149794164112], ["Language Identification", 0.17127324228676358], ["Machine Reading Comprehension", 0.16434411073224484]], "link": [{"@href": "http://arxiv.org/abs/2402.06501v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06501v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06499v1", "title": "BarlowTwins-CXR : Enhancing Chest X-Ray abnormality localization in\n  heterogeneous data with cross-domain self-supervised learning", "summary": "Background: Chest X-ray imaging-based abnormality localization, essential in\ndiagnosing various diseases, faces significant clinical challenges due to\ncomplex interpretations and the growing workload of radiologists. While recent\nadvances in deep learning offer promising solutions, there is still a critical\nissue of domain inconsistency in cross-domain transfer learning, which hampers\nthe efficiency and accuracy of diagnostic processes. This study aims to address\nthe domain inconsistency problem and improve autonomic abnormality localization\nperformance of heterogeneous chest X-ray image analysis, by developing a\nself-supervised learning strategy called \"BarlwoTwins-CXR\". Methods: We\nutilized two publicly available datasets: the NIH Chest X-ray Dataset and the\nVinDr-CXR. The BarlowTwins-CXR approach was conducted in a two-stage training\nprocess. Initially, self-supervised pre-training was performed using an\nadjusted Barlow Twins algorithm on the NIH dataset with a Resnet50 backbone\npre-trained on ImageNet. This was followed by supervised fine-tuning on the\nVinDr-CXR dataset using Faster R-CNN with Feature Pyramid Network (FPN).\nResults: Our experiments showed a significant improvement in model performance\nwith BarlowTwins-CXR. The approach achieved a 3% increase in mAP50 accuracy\ncompared to traditional ImageNet pre-trained models. In addition, the Ablation\nCAM method revealed enhanced precision in localizing chest abnormalities.\nConclusion: BarlowTwins-CXR significantly enhances the efficiency and accuracy\nof chest X-ray image-based abnormality localization, outperforming traditional\ntransfer learning methods and effectively overcoming domain inconsistency in\ncross-domain scenarios. Our experiment results demonstrate the potential of\nusing self-supervised learning to improve the generalizability of models in\nmedical settings with limited amounts of heterogeneous data.", "author": null, "tags": [["Language Identification", 0.16604128053239153]], "link": [{"@href": "http://arxiv.org/abs/2402.06499v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06499v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06497v1", "title": "Iris-SAM: Iris Segmentation Using a Foundational Model", "summary": "Iris segmentation is a critical component of an iris biometric system and it\ninvolves extracting the annular iris region from an ocular image. In this work,\nwe develop a pixel-level iris segmentation model from a foundational model,\nviz., Segment Anything Model (SAM), that has been successfully used for\nsegmenting arbitrary objects. The primary contribution of this work lies in the\nintegration of different loss functions during the fine-tuning of SAM on ocular\nimages. In particular, the importance of Focal Loss is borne out in the\nfine-tuning process since it strategically addresses the class imbalance\nproblem (i.e., iris versus non-iris pixels). Experiments on ND-IRIS-0405,\nCASIA-Iris-Interval-v3, and IIT-Delhi-Iris datasets convey the efficacy of the\ntrained model for the task of iris segmentation. For instance, on the\nND-IRIS-0405 dataset, an average segmentation accuracy of 99.58% was achieved,\ncompared to the best baseline performance of 89.75%.", "author": null, "tags": [["Text Segmentation", 0.28044849712086883]], "link": [{"@href": "http://arxiv.org/abs/2402.06497v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06497v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06496v1", "title": "Benchmarking ionization potentials from pCCD tailored coupled cluster\n  models", "summary": "The ionization potential (IP) is an important parameter providing essential\ninsights into the reactivity of chemical systems. IPs are also crucial for\ndesigning, optimizing, and understanding the functionality of modern\ntechnological devices. We recently showed that limiting the CC ansatz to the\nseniority-zero sector proves insufficient in predicting reliable and accurate\nionization potentials within an IP equation-of-motion coupled-cluster\nformalism. Specifically, the absence of dynamic correlation in the\nseniority-zero pair coupled cluster doubles (pCCD) model led to unacceptably\nsignificant errors of approximately 1.5 eV. In this work, we aim to explore the\nimpact of dynamical correlation and the choice of the molecular orbital basis\n(canonical vs. localized) in CC-type methods targeting 201 ionized states in 41\nmolecules. We focus on pCCD-based approaches as well as the conventional\nIP-EOM-CCD and IP-EOM-CCSD. Their performance is compared to the CCSDT\nequivalent and experimental reference data. Our statistical analysis reveals\nthat all investigated frozen-pair coupled cluster methods exhibit similar\nperformance, with differences in errors typically within chemical accuracy (1\nkcal/mol or 0.05 eV). Notably, the effect of the molecular orbital basis, such\nas canonical Hartree-Fock or natural pCCD-optimized orbitals, on the IPs is\nmarginal if dynamical correlation is accounted for. Our study suggests that\ntriple excitations are crucial in achieving chemical accuracy in IPs when\nmodeling electron detachment processes with pCCD-based methods.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06496v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06496v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06492v1", "title": "Inducing Systematicity in Transformers by Attending to Structurally\n  Quantized Embeddings", "summary": "Transformers generalize to novel compositions of structures and entities\nafter being trained on a complex dataset, but easily overfit on datasets of\ninsufficient complexity. We observe that when the training set is sufficiently\ncomplex, the model encodes sentences that have a common syntactic structure\nusing a systematic attention pattern. Inspired by this observation, we propose\nSQ-Transformer (Structurally Quantized) that explicitly encourages\nsystematicity in the embeddings and attention layers, even with a training set\nof low complexity. At the embedding level, we introduce Structure-oriented\nVector Quantization (SoVQ) to cluster word embeddings into several classes of\nstructurally equivalent entities. At the attention level, we devise the\nSystematic Attention Layer (SAL) and an alternative, Systematically Regularized\nLayer (SRL) that operate on the quantized word embeddings so that sentences of\nthe same structure are encoded with invariant or similar attention patterns.\nEmpirically, we show that SQ-Transformer achieves stronger compositional\ngeneralization than the vanilla Transformer on multiple low-complexity semantic\nparsing and machine translation datasets. In our analysis, we show that SoVQ\nindeed learns a syntactically clustered embedding space and SAL/SRL induces\ngeneralizable attention patterns, which lead to improved systematicity.", "author": null, "tags": [["Word Embeddings", 0.3404496531055172], ["Document Clustering", 0.1784023507067514], ["Syntactic Analysis (Parsing)", 0.16998974172796047]], "link": [{"@href": "http://arxiv.org/abs/2402.06492v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06492v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06490v1", "title": "Heavy element abundances in Galactic Globular Clusters", "summary": "Context. Globular clusters are considered key objects for understanding the\nformation and evolution of the Milky Way. In this sense, their characterisation\nin terms of their chemical and orbital parameters can provide constraints to\nthe chemical evolution models of the Galaxy. Aims. We use the heavy element\nabundances of globular clusters to trace their overall behaviour in the Galaxy,\naiming to analyse potential relations between the hot H-burning and s-process\nelements. Methods. We measured the content of Cu I and s- and r-process\nelements (Y II, Ba II, La II, and Eu II) in a sample of 210 giant stars in 18\nGalactic Globular Clusters from high-quality UVES spectra. The clusters span a\nlarge metallicity range, and the sample is the largest uniformly analysed for\nwhat concerns heavy elements in Globular Clusters. Results. Cu abundances did\nnot show considerable spread in the sample nor correlation with Na, meaning\nthat the Na nucleosynthesis process does not affect the Cu abundance. Most GCs\nclosely follow the Cu, Y, Ba, La, and Eu field stars' distribution, revealing a\nsimilar chemical evolution. The Y abundances in mid-metallicity regime GCs\n(-1.10 dex <[Fe/H]<-1.80 dex) display a mildly significant correlation with the\nNa abundance, which should be further investigated. Finally, we did not find\nany significant difference between the n-capture abundances among GCs with\nGalactic and extragalactic origin.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06490v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06490v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06487v1", "title": "Le Nozze di Giustizia. Interactions between Artificial Intelligence,\n  Law, Logic, Language and Computation with some case studies in Traffic\n  Regulations and Health Care", "summary": "An important aim of this paper is to convey some basics of mathematical logic\nto the legal community working with Artificial Intelligence. After analysing\nwhat AI is, we decide to delimit ourselves to rule-based AI leaving Neural\nNetworks and Machine Learning aside. Rule based AI allows for Formal methods\nwhich are described in a rudimentary form. We will then see how mathematical\nlogic interacts with legal rule-based AI practice. We shall see how\nmathematical logic imposes limitations and complications to AI applications. We\nclassify the limitations and interactions between mathematical logic and legal\nAI in three categories: logical, computational and mathematical. The examples\nto showcase the interactions will largely come from European traffic\nregulations. The paper closes off with some reflections on how and where AI\ncould be used and on basic mechanisms that shape society.", "author": null, "tags": [["Text Wrangling", 0.18893410247067802], ["Chatbot Development", 0.1690153661367846]], "link": [{"@href": "http://arxiv.org/abs/2402.06487v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06487v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06485v1", "title": "The structural evolution of temporal hypergraphs through the lens of\n  hyper-cores", "summary": "The richness of many complex systems stems from the interactions among their\ncomponents. The higher-order nature of these interactions, involving many units\nat once, and their temporal dynamics constitute crucial properties that shape\nthe behaviour of the system itself. An adequate description of these systems is\noffered by temporal hypergraphs, that integrate these features within the same\nframework. However, tools for their temporal and topological characterization\nare still scarce. Here we develop a series of methods specifically designed to\nanalyse the structural properties of temporal hypergraphs at multiple scales.\nLeveraging the hyper-core decomposition of hypergraphs, we follow the evolution\nof the hyper-cores through time, characterizing the hypergraph structure and\nits temporal dynamics at different topological scales, and quantifying the\nmulti-scale structural stability of the system. We also define two static\nhypercoreness centrality measures that provide an overall description of the\nnodes aggregated structural behaviour. We apply the characterization methods to\nseveral data sets, establishing connections between structural properties and\nspecific activities within the systems. Finally, we show how the proposed\nmethod can be used as a model-validation tool for synthetic temporal\nhypergraphs, distinguishing the higher-order structures and dynamics generated\nby different models from the empirical ones, and thus identifying the essential\nmodel mechanisms to reproduce the empirical hypergraph structure and evolution.\nOur work opens several research directions, from the understanding of dynamic\nprocesses on temporal higher-order networks to the design of new models of\ntime-varying hypergraphs.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06485v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06485v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06475v1", "title": "Large Language Models for Captioning and Retrieving Remote Sensing\n  Images", "summary": "Image captioning and cross-modal retrieval are examples of tasks that involve\nthe joint analysis of visual and linguistic information. In connection to\nremote sensing imagery, these tasks can help non-expert users in extracting\nrelevant Earth observation information for a variety of applications. Still,\ndespite some previous efforts, the development and application of vision and\nlanguage models to the remote sensing domain have been hindered by the\nrelatively small size of the available datasets and models used in previous\nstudies. In this work, we propose RS-CapRet, a Vision and Language method for\nremote sensing tasks, in particular image captioning and text-image retrieval.\nWe specifically propose to use a highly capable large decoder language model\ntogether with image encoders adapted to remote sensing imagery through\ncontrastive language-image pre-training. To bridge together the image encoder\nand language decoder, we propose training simple linear layers with examples\nfrom combining different remote sensing image captioning datasets, keeping the\nother parameters frozen. RS-CapRet can then generate descriptions for remote\nsensing images and retrieve images from textual descriptions, achieving SOTA or\ncompetitive performance with existing methods. Qualitative results illustrate\nthat RS-CapRet can effectively leverage the pre-trained large language model to\ndescribe remote sensing images, retrieve them based on different types of\nqueries, and also show the ability to process interleaved sequences of images\nand text in a dialogue manner.", "author": null, "tags": [["Text Segmentation", 0.25127791064742094], ["Multimodal Language Processing", 0.17386405523109927]], "link": [{"@href": "http://arxiv.org/abs/2402.06475v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06475v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06469v1", "title": "Deuterated Polystyrene -- Synthesis and uses for ultracold neutron\n  bottles and the neutron EDM experiment", "summary": "The synthesis and application of deuterated polystyrene (dps) films is\ndiscussed. Ultracold neutron storage properties and the Fermi potential of dps\nfilms is measured with the result that Tstore=700 +/- 200 sec for dps in the\nbottle used and the Fermi potential is about 165neV. The behavior under\napplication of high electric fields in vacuum is measured; the films are\nsufficiently stable to use in a neutron EDM bottle. Also, the relaxation rate\nof nuclear spin polarized 199Hg on dps films is measured giving a wall lifetime\nof 20 sec/cm mean free path, which should make the development of an Hg volume\nmagnetometer possible.", "author": ["Steve K. Lamoreaux"], "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06469v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06469v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06468v1", "title": "Flexible, photonic films of surfactant-functionalized cellulose\n  nanocrystals for pressure and humidity sensing", "summary": "Most paints contain pigments that absorb light and fade over time. A robust\nalternative can be found in nature, where structural coloration arises from the\ninterference of light with submicron features. Plant-derived, cellulose\nnanocrystals (CNCs) mimic these features by self-assembling into a cholesteric\nliquid crystal that exhibits structural coloration when dried. While much\nresearch has been done on CNCs in aqueous solutions, less is known about\ntransferring CNCs to apolar solvents that are widely employed in paints. This\nstudy uses a common surfactant in agricultural and industrial products to\nsuspend CNCs in toluene that are then dried into structurally colored films.\nSurprisingly, a stable liquid crystal phase is formed within hours, even with\nconcentrations of up to 50 wt.-%. Evaporating the apolar CNC suspensions\nresults in photonic films with peak wavelengths ranging from 660 to 920 nm. The\nresulting flexible films show increased mechanical strength, enabling a\nblue-shift into the visible spectrum with applied force. The films also act as\nhumidity sensors, with increasing relative humidity yielding a red-shift. With\nthe addition of a single surfactant, CNCs can be made compatible with existing\nproduction methods of industrial coatings, while improving the strength and\nresponsiveness of structurally-colored films to external stimuli.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06468v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06468v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06464v1", "title": "Penning-trap measurement of the $Q$-value of the electron capture in\n  $^{163}\\mathrm{Ho}$ for the determination of the electron neutrino mass", "summary": "The investigation of the absolute scale of the effective neutrino mass\nremains challenging due to the exclusively weak interaction of neutrinos with\nall known particles in the standard model of particle physics. Currently, the\nmost precise and least model-dependent upper limit on the electron antineutrino\nmass is set by the KATRIN experiment from the analysis of the tritium\n\\b{eta}-decay. Another promising approach is the electron capture in\n$^{163}\\mathrm{Ho}$, which is under investigation using microcalorimetry within\nthe ECHo and HOLMES collab orations. An independently measured Q-value of this\nprocess is vital for the assessment of systematic uncertainties in the neutrino\nmass determination. Here, we report a direct, independent determination of this\n$Q$-value by measuring the free-space cyclotron frequency ratio of highly\ncharged ions of $^{163}\\mathrm{Ho}$ and $^{163}\\mathrm{Dy}$ in the Penning trap\nexperiment \\textsc{Pentatrap}. Combining this ratio with atomic physics\ncalculations of the electronic binding energies yields a $Q$-value of\n$2863.2(0.6)\\,\\mathrm{eV}/c^{2}$ - a more than 50-fold improvement over the\nstate-of-the-art. This will enable the determination of the electron neutrino\nmass on a sub-eV level from the analysis of the electron capture in\n$^{163}\\mathrm{Ho}$.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06464v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06464v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06463v1", "title": "Cardiac ultrasound simulation for autonomous ultrasound navigation", "summary": "Ultrasound is well-established as an imaging modality for diagnostic and\ninterventional purposes. However, the image quality varies with operator skills\nas acquiring and interpreting ultrasound images requires extensive training due\nto the imaging artefacts, the range of acquisition parameters and the\nvariability of patient anatomies. Automating the image acquisition task could\nimprove acquisition reproducibility and quality but training such an algorithm\nrequires large amounts of navigation data, not saved in routine examinations.\nThus, we propose a method to generate large amounts of ultrasound images from\nother modalities and from arbitrary positions, such that this pipeline can\nlater be used by learning algorithms for navigation. We present a novel\nsimulation pipeline which uses segmentations from other modalities, an\noptimized volumetric data representation and GPU-accelerated Monte Carlo path\ntracing to generate view-dependent and patient-specific ultrasound images. We\nextensively validate the correctness of our pipeline with a phantom experiment,\nwhere structures' sizes, contrast and speckle noise properties are assessed.\nFurthermore, we demonstrate its usability to train neural networks for\nnavigation in an echocardiography view classification experiment by generating\nsynthetic images from more than 1000 patients. Networks pre-trained with our\nsimulations achieve significantly superior performance in settings where large\nreal datasets are not available, especially for under-represented classes. The\nproposed approach allows for fast and accurate patient-specific ultrasound\nimage generation, and its usability for training networks for\nnavigation-related tasks is demonstrated.", "author": null, "tags": [["Text Segmentation", 0.26270159778795077], ["Information Extraction", 0.1768595041322314], ["Text Normalization", 0.17483411666590423]], "link": [{"@href": "http://arxiv.org/abs/2402.06463v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06463v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06462v1", "title": "Crustal Structure Across the Northern Region of the Islas Marias\n  Archipelago", "summary": "The tectonic interaction between the Rivera and North American plates north\nof the Bahia de Banderas is poorly understood. The nature of the crust and\nwhere the subduction ends in the western part of the Islas Marias Archipelago\nare still controversial. Based on new geophysical data provided by the TsuJal\nproject, we present the shallow and deep crustal structure of the Rivera North\nAmerican plate contact zone along two seismic transects, TS09b and RTSIM01b,\nand the bathymetry obtained across the northern region of Maria Madre Island.\nDetailed bathymetric analysis allowed mapping of a series of lineaments along\nthe study region, with two main preferred tendencies (020 050 and 290 320)\nassociated with the evolution of the Pacific-Rivera rise and the transform\nfaults of the Gulf of California, respectively. The shallow structure is\ncharacterized by five sedimentary basins without deformation, whose horizons\nare subparallel, suggesting that the sediment deposition occurred after the\nextension process ended. The deep structure corresponds to a transition between\noceanic crust (Rivera Plate), with an average thickness of 10 km to the Islas\nMarias Escarpment, and a thinned continental crust, whose thickness increases\ntoward the continent until it reaches 28 km, with a dip angle of 7 10. The\nabsence of an accretionary prism suggests that the subduction process of the\nRivera Plate beneath the North American Plate to the north of Islas Marias has\nceased. In this study, we determined that the morphological expression of the\nnorthern limit of the Rivera Plate corresponds to the Islas Marias Escarpment.", "author": null, "tags": [], "link": [{"@title": "doi", "@href": "http://dx.doi.org/10.3389/feart.2021.682206", "@rel": "related"}, {"@href": "http://arxiv.org/abs/2402.06462v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06462v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06457v1", "title": "V-STaR: Training Verifiers for Self-Taught Reasoners", "summary": "Common self-improvement approaches for large language models (LLMs), such as\nSTaR (Zelikman et al., 2022), iteratively fine-tune LLMs on self-generated\nsolutions to improve their problem-solving ability. However, these approaches\ndiscard the large amounts of incorrect solutions generated during this process,\npotentially neglecting valuable information in such solutions. To address this\nshortcoming, we propose V-STaR that utilizes both the correct and incorrect\nsolutions generated during the self-improvement process to train a verifier\nusing DPO that judges correctness of model-generated solutions. This verifier\nis used at inference time to select one solution among many candidate\nsolutions. Running V-STaR for multiple iterations results in progressively\nbetter reasoners and verifiers, delivering a 4% to 17% test accuracy\nimprovement over existing self-improvement and verification approaches on\ncommon code generation and math reasoning benchmarks with LLaMA2 models.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06457v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06457v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06453v1", "title": "Optical realization of magneto-intersubband oscillations", "summary": "We report on the optical realization of the magneto-intersubband oscillations\nthat have been measured in the sub-terahertz transmittance of a GaAs quantum\nwell with two subbands occupied. Following their dc analogue, the oscillations\nare periodic in the inverse magnetic field with the period governed by the\nsubband gap. Their magnitude and polarization dependence accurately follow the\npresented simplified version of the dynamic magneto-intersubband oscillations\nequation that naturally combines dc magneto-intersabband oscillations with\nmicrowave-induced resistance oscillations (MIRO). Simultaneously measured\nphotoresistance also reveals its strong sensitivity to the sign of the circular\npolarization, proving the used theoretical modeling.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06453v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06453v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06452v1", "title": "An Algorithmic Framework for Constructing Multiple Decision Trees by\n  Evaluating Their Combination Performance Throughout the Construction Process", "summary": "Predictions using a combination of decision trees are known to be effective\nin machine learning. Typical ideas for constructing a combination of decision\ntrees for prediction are bagging and boosting. Bagging independently constructs\ndecision trees without evaluating their combination performance and averages\nthem afterward. Boosting constructs decision trees sequentially, only\nevaluating a combination performance of a new decision tree and the fixed past\ndecision trees at each step. Therefore, neither method directly constructs nor\nevaluates a combination of decision trees for the final prediction. When the\nfinal prediction is based on a combination of decision trees, it is natural to\nevaluate the appropriateness of the combination when constructing them. In this\nstudy, we propose a new algorithmic framework that constructs decision trees\nsimultaneously and evaluates their combination performance throughout the\nconstruction process. Our framework repeats two procedures. In the first\nprocedure, we construct new candidates of combinations of decision trees to\nfind a proper combination of decision trees. In the second procedure, we\nevaluate each combination performance of decision trees under some criteria and\nselect a better combination. To confirm the performance of the proposed\nframework, we perform experiments on synthetic and benchmark data.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06452v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06452v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06451v1", "title": "Competitive and Weighted Evolving Simplicial Complexes", "summary": "A simplex-based network is referred to as a higher-order network, in which\ndescribe that the interactions can include more than two nodes. This paper\nfirst proposes a competitive evolving model of higher-order networks. We notice\nthe batch effect of low-dim simplices during the growth of such a network. We\nobtain an analytical expression for the distribution of higher-order degrees by\nemploying the theory of Poisson processes and the mean field method and use\ncomputers to simulate higher-order networks of competitions. The established\nresults indicate that the scale-free behavior for the (d-1)-dim simplex with\nrespect to the d-order degree is controlled by the competitiveness factor. As\nthe competitiveness increases, the d-order degree of the (d-1)-dim simplex is\nbent under the logarithmic coordinates. Second, by considering the weight\nchanges of the neighboring simplices, as triggered by the selected simplex, a\nnew weighted evolving model in higher-order networks is proposed. The results\nof the competitive evolving model of higher-order networks are used to analyze\nthe weighted evolving model so that obtained are the analytical expressions of\nthe higher-order degree distribution and higher-order strength density function\nof weighted higher-order networks. The outcomes of the simulation experiments\nare consistent with the theoretical analysis. Therefore, the weighted network\nbelongs to the collection of competition networks.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06451v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06451v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06449v1", "title": "The EBLM project -- XIII. The absolute dynamical masses of the\n  circumbinary planet host TOI-1338/BEBOP-1", "summary": "High-contrast eclipsing binaries with low mass M-dwarf secondaries are\nprecise benchmark stars to build empirical mass-radius relationships for fully\nconvective low-mass ($\\rm M_{*} < 0.35\\,M_{\\rm sun}$) dwarf stars. The\ncontributed light of the M-dwarf in such binaries is usually much less than\none~per~cent at optical wavelengths. This enables the detection of circumbinary\nplanets from precise radial velocity measurements. High-resolution\ncross-correlation techniques are typically used to detect exoplanet\natmospheres. One key aspect of these techniques is the post-processing, which\nincludes the removal of telluric and spectral lines of the host star. We\nintroduce the application of such techniques to optical high-resolution spectra\nof the circumbinary planet-host TOI-1338/BEBOP-1, turning it effectively into a\ndouble-lined eclipsing binary. By using simulations, we further explore the\nimpact of post-processing techniques for high-contrast systems. We detect the\nM-dwarf secondary with a significance of 11-$\\sigma$ and measure absolute\ndynamical masses for both components. Compared to previous model-dependent mass\nmeasurements, we obtain a four times better precision. We further find that the\npost-processing results in negligible systematic impact on the radial velocity\nprecision for TOI-1338/BEBOP-1 with more than $96.6\\,$per~cent (1-$\\sigma$) of\nthe M-dwarf's signal being conserved. We show that these methods can be used to\nrobustly measure dynamical masses of high-contrast single-lined binaries\nproviding important benchmark stars for stellar evolution particularly near the\nbottom of the main sequence. We also demonstrate how to retrieve the phase\ncurve of an exoplanet with high-resolution spectroscopy using our data.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06449v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06449v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06443v1", "title": "Explaining Veracity Predictions with Evidence Summarization: A\n  Multi-Task Model Approach", "summary": "The rapid dissemination of misinformation through social media increased the\nimportance of automated fact-checking. Furthermore, studies on what deep neural\nmodels pay attention to when making predictions have increased in recent years.\nWhile significant progress has been made in this field, it has not yet reached\na level of reasoning comparable to human reasoning. To address these gaps, we\npropose a multi-task explainable neural model for misinformation detection.\nSpecifically, this work formulates an explanation generation process of the\nmodel's veracity prediction as a text summarization problem. Additionally, the\nperformance of the proposed model is discussed on publicly available datasets\nand the findings are evaluated with related studies.", "author": null, "tags": [["Fake News Detection", 0.1808949010593097]], "link": [{"@href": "http://arxiv.org/abs/2402.06443v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06443v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06430v1", "title": "Polyarc bounded complex interval arithmetic", "summary": "Complex interval arithmetic is a powerful tool for the analysis of\ncomputational errors. The naturally arising rectangular, polar, and circular\n(together called primitive) interval types are not closed under simple\narithmetic operations and their use yields overly relaxed bounds. The later\nintroduced polygonal type, on the other hand, allows for arbitrarily precise\nrepresentaion of the above operations for a higher computational cost. We\npropose the polyarcular interval type as an effective extension of the previous\ntypes. The polyarcular interval can represent all primitive intervals and most\nof their arithmetic combinations precisely and has a approximation capability\ncompeting with that of the polygonal interval. In particular, in antenna\ntolerance analysis it can achieve perfect accuracy for lower computational cost\nthen the polygonal type, which we show in a relevant case study. In this paper,\nwe present a rigorous analysis of the arithmetic properties of all five\ninterval types, involving a new algebro-geometric method of boundary analysis.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06430v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06430v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06427v1", "title": "Quantum thermodynamics with a single superconducting vortex", "summary": "We demonstrate complete control over dynamics of a single superconducting\nvortex in a nanostructure which we coin the Single Vortex Box (SVB). Our device\nallows us to trap the vortex in a field-cooled aluminum nanosquare and expel it\non demand with a nanosecond pulse of electrical current. We read-out the vortex\nstate of the box by testing the switching current of the adjacent Dayem\nnanobridge. Using the time-resolving nanothermometry we measure\n4$\\cdot$10$^{-19}\\,$J as the amount of the dissipated heat (which is the energy\nof a single red photon) in the elementary process of the vortex expulsion, and\nmonitor the following thermal relaxation of the device. The measured heat is\nequal to the energy required to annihilate all Cooper pairs on the way of the\nmoving vortex. Our design and measuring protocol are convenient for studying\nthe stochastic mechanism of the vortex escape from current-driven\nsuperconducting nanowires, which has its roots either in thermal or quantum\nfluctuations, similar to ones widely studied in Josephson junctions or magnetic\nnanoclusters and molecules. Our experiment enlightens the thermodynamics of the\nabsorption process in the superconducting nanowire single-photon detectors, in\nwhich vortices are perceived to be essential for a formation of a detectable\nhot spot. The demonstrated opportunity to manipulate a single superconducting\nvortex reliably in a confined geometry comprises in fact a proof-of-concept of\na nanoscale non-volatile memory cell with sub-nanosecond write and read\noperations, which offers compatibility with quantum processors based either on\nsuperconducting qubits or rapid single flux quantum circuits.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06427v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06427v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06423v1", "title": "CurveFormer++: 3D Lane Detection by Curve Propagation with Temporal\n  Curve Queries and Attention", "summary": "In autonomous driving, 3D lane detection using monocular cameras is an\nimportant task for various downstream planning and control tasks. Recent CNN\nand Transformer approaches usually apply a two-stage scheme in the model\ndesign. The first stage transforms the image feature from a front image into a\nbird's-eye-view (BEV) representation. Subsequently, a sub-network processes the\nBEV feature map to generate the 3D detection results. However, these approaches\nheavily rely on a challenging image feature transformation module from a\nperspective view to a BEV representation. In our work, we present\nCurveFormer++, a single-stage Transformer-based method that does not require\nthe image feature view transform module and directly infers 3D lane detection\nresults from the perspective image features. Specifically, our approach models\nthe 3D detection task as a curve propagation problem, where each lane is\nrepresented by a curve query with a dynamic and ordered anchor point set. By\nemploying a Transformer decoder, the model can iteratively refine the 3D lane\ndetection results. A curve cross-attention module is introduced in the\nTransformer decoder to calculate similarities between image features and curve\nqueries of lanes. To handle varying lane lengths, we employ context sampling\nand anchor point restriction techniques to compute more relevant image features\nfor a curve query. Furthermore, we apply a temporal fusion module that\nincorporates selected informative sparse curve queries and their corresponding\nanchor point sets to leverage historical lane information. In the experiments,\nwe evaluate our approach for the 3D lane detection task on two publicly\navailable real-world datasets. The results demonstrate that our method provides\noutstanding performance compared with both CNN and Transformer based methods.\nWe also conduct ablation studies to analyze the impact of each component in our\napproach.", "author": null, "tags": [["Text Segmentation", 0.24143024617315378], ["Fake News Detection", 0.21996681040436752], ["Information Extraction", 0.21444788691757596], ["Text Normalization", 0.20496283795735395], ["Paraphrase Detection", 0.19329871041307947], ["Sarcasm Detection", 0.18814277677066166], ["Relation Extraction", 0.18645795057764938], ["Multimodal Language Processing", 0.1813912856055597]], "link": [{"@href": "http://arxiv.org/abs/2402.06423v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06423v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06422v1", "title": "Determining Strain Components in a Diamond Waveguide from Zero-Field\n  ODMR Spectra of NV$^{-}$ Center Ensembles", "summary": "The negatively charged nitrogen-vacancy (NV${}^-$) center in diamond has\nshown great potential in nanoscale sensing and quantum information processing\ndue to its rich spin physics. An efficient coupling with light, providing\nstrong luminescence, is crucial for realizing these applications. Laser-written\nwaveguides in diamond promote NV${}^-$ creation and improve their coupling to\nlight but at the same time induce strain in the crystal. The induced strain\ncontributes to light guiding but also affects the energy levels of NV${}^-$\ncenters. We probe NV${}^-$ spin states experimentally with the commonly used\nzero-field optically detected magnetic resonance (ODMR). In our waveguides, the\nODMR spectra are shifted, split, and consistently asymmetric, which we\nattribute to the impact of local strain. To understand these features, we model\nensemble ODMR signals in the presence of strain. By fitting the model results\nto the experimentally collected ODMR data we determine the strain tensor\ncomponents at different positions, thus determining the strain profile across\nthe waveguide. We show that ODMR spectroscopy can be used as a strain imaging\ntool. The resulting strain within the waveguide is dominated by a compressive\naxial component transverse to the waveguide structure, with a smaller\ncontribution from vertical and shear strain components.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06422v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06422v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06420v1", "title": "Findings of the First Workshop on Simulating Conversational Intelligence\n  in Chat", "summary": "The aim of this workshop is to bring together experts working on open-domain\ndialogue research. In this speedily advancing research area many challenges\nstill exist, such as learning information from conversations, engaging in\nrealistic and convincing simulation of human intelligence and reasoning.\nSCI-CHAT follows previous workshops on open domain dialogue but with a focus on\nthe simulation of intelligent conversation as judged in a live human\nevaluation. Models aim to include the ability to follow a challenging topic\nover a multi-turn conversation, while positing, refuting and reasoning over\narguments. The workshop included both a research track and shared task. The\nmain goal of this paper is to provide an overview of the shared task and a link\nto an additional paper that will include an in depth analysis of the shared\ntask results following presentation at the workshop.", "author": null, "tags": [["Dialogue Systems", 0.17632990031036314]], "link": [{"@href": "http://arxiv.org/abs/2402.06420v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06420v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06415v1", "title": "Many-body computing on Field Programmable Gate Arrays", "summary": "A new implementation of many-body calculations is of paramount importance in\nthe field of computational physics. In this study, we leverage the capabilities\nof Field Programmable Gate Arrays (FPGAs) for conducting quantum many-body\ncalculations. Through the design of appropriate schemes for Monte Carlo and\ntensor network methods, we effectively utilize the parallel processing\ncapabilities provided by FPGAs. This has resulted in a remarkable tenfold\nspeedup compared to CPU-based computation for a Monte Carlo algorithm. We also\ndemonstrate, for the first time, the utilization of FPGA to accelerate a\ntypical tensor network algorithm. Our findings unambiguously highlight the\nsignificant advantages of hardware implementation and pave the way for novel\napproaches to many-body calculations.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06415v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06415v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06414v1", "title": "Trust the Process: Zero-Knowledge Machine Learning to Enhance Trust in\n  Generative AI Interactions", "summary": "Generative AI, exemplified by models like transformers, has opened up new\npossibilities in various domains but also raised concerns about fairness,\ntransparency and reliability, especially in fields like medicine and law. This\npaper emphasizes the urgency of ensuring fairness and quality in these domains\nthrough generative AI. It explores using cryptographic techniques, particularly\nZero-Knowledge Proofs (ZKPs), to address concerns regarding performance\nfairness and accuracy while protecting model privacy. Applying ZKPs to Machine\nLearning models, known as ZKML (Zero-Knowledge Machine Learning), enables\nindependent validation of AI-generated content without revealing sensitive\nmodel information, promoting transparency and trust. ZKML enhances AI fairness\nby providing cryptographic audit trails for model predictions and ensuring\nuniform performance across users. We introduce snarkGPT, a practical ZKML\nimplementation for transformers, to empower users to verify output accuracy and\nquality while preserving model privacy. We present a series of empirical\nresults studying snarkGPT's scalability and performance to assess the\nfeasibility and challenges of adopting a ZKML-powered approach to capture\nquality and performance fairness problems in generative AI models.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06414v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06414v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06411v1", "title": "Exploiting spatial diversity for increasing the robustness of sound\n  source localization systems against reverberation", "summary": "Acoustic reverberation is one of the most relevant factors that hampers the\nlocalization of a sound source inside a room. To date, several approaches have\nbeen proposed to deal with it, but have not always been evaluated under\nrealistic conditions. This paper proposes exploiting spatial diversity as an\nalternative approach to achieve robustness against reverberation. The\ntheoretical arguments supporting this approach are first presented and later\nconfirmed by means of simulation results and real measurements. Simulations are\nrun for reverberation times up to 2 s, thus providing results with a wider\nrange of validity than in other previous research works. It is concluded that\nthe use of systems consisting of several, sufficiently separated, small arrays\nleads to the best results in reverberant environments. Some recommendations are\ngiven regarding the choice of the array sizes, the separation among them, and\nthe way to combine SRP-PHAT maps obtained from diverse arrays.", "author": null, "tags": [], "link": [{"@title": "doi", "@href": "http://dx.doi.org/10.1016/j.apacoust.2022.109138", "@rel": "related"}, {"@href": "http://arxiv.org/abs/2402.06411v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06411v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06409v1", "title": "Enhanced bubble growth near an advancing solidification front", "summary": "Frozen water might appear opaque since impurities or gas bubbles can get\ntrapped in the ice during the freezing process. The latter nucleate and then\ngrow near the advancing solidification front, due to the formation of a gas\nsupersaturation region in its vicinity. A delicate interplay between the rate\nof mass transfer and the rate of freezing dictates the final shapes and sizes\nof the entrapped gas bubbles. In this work, we experimentally and numerically\ninvestigate the initial growth of such gas bubbles that nucleate and grow near\nthe advancing ice front. We show that the initial growth of these bubbles is\ngoverned by diffusion and is enhanced due to a combination of the presence of\nthe background gas concentration gradient and the motion of the approaching\nfront. Additionally, we recast the problem into that of mass transfer to a\nmoving spherical object in a homogeneous concentration field, finding good\nagreement between our experimental data and the existing scaling relations for\nthat latter problem. Lastly, we address how fluid flow around the bubble might\nfurther affect this growth and qualitatively explore this through numerical\nsimulations.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06409v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06409v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06401v1", "title": "Differential inclusions, polycrystals and stability under lamination", "summary": "We study approximate solutions to a differential inclusion associated to a\ncertain system of pdes in dimension three. The only datum is a set of three\npositive numbers identified with a positive definite diagonal matrix $S$. The\naverage fields naturally belong to the convex hull of the set of points\nobtained by the triple $S$ and its permutations. We find a set of attainable\naverage fields strictly contained in the convex hull and stable under\nlamination. The corresponding microgeometries are laminates of infinite rank\nwhich have an algebraic characterization that may be of independent interest.\nThe original motivation comes from polycrystalline linearly conducting\ncomposites. As a by-product, our result establishes the optimality of a large\nclass of microgeometries for the effective conductivity of such materials.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06401v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06401v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06395v1", "title": "A Statistical Model of Bursty Mixed Gaussian-impulsive Noise: Model and\n  Parameter Estimation", "summary": "Non-Gaussian impulsive noise (IN) with memory exists in many practical\napplications. When it is mixed with white Gaussian noise (WGN), the resultant\nmixed noise will be bursty. The performance of communication systems will\ndegrade significantly under bursty mixed noise if the bursty characteristic is\nignored. A proper model for the bursty mixed noise and corresponding algorithms\nneeds to be designed to obtain desirable performance but there is no such model\nreported to the best of our knowledge. The important problem is addressed in\nthe two-part paper. In the first part, we propose a closed-form heavy-tailed\nmultivariate probability density function (PDF) that to model the bursty mixed\nnoise. This model is the weighted addition of gaussian distribution and student\ndistribution. Then, we present the parameter estimation method based on the\nempirical characteristic function of the proposed model and analyze the\nperformance of the parameter estimation. Numerical results show that our\nproposed bursty mixed noise model matches the measured bursty noise well.\nMeanwhile, the parameters of the proposed noise model can be accurately\nestimated in terms of mean square error (MSE).", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06395v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06395v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06393v1", "title": "Stellar streams from black hole-rich star clusters", "summary": "Nearly a hundred progenitor-less, thin stellar streams have been discovered\nin the Milky Way, thanks to Gaia and related surveys. Most streams are believed\nto have formed from star clusters and it was recently proposed that extended\nstar clusters -- rich in stellar-mass black holes (BHs) -- are efficient in\ncreating streams. To understand the nature of stream progenitors better, we\nquantify the differences between streams originating from star clusters with\nand without BHs using direct $N$-body models and a new model for the density\nprofiles of streams based on time-dependent escape rates from clusters. The QSG\n(Quantifying Stream Growth) model facilitates the rapid exploration of\nparameter space and provides an analytic framework to understand the impact of\ndifferent star cluster properties and escape conditions on the structure of\nstreams. Using these models it is found that, compared to streams from BH-free\nclusters on the same orbit, streams of BH-rich clusters: (1) are approximately\nfive times more massive; (2) have a peak density three times closer to the\ncluster 1 Gyr post-dissolution (for orbits of Galactocentric radius > 10 kpc),\nand (3) have narrower peaks and more extended wings in their density profile.\nWe discuss other observable stream properties that are affected by the presence\nof BHs in their progenitor cluster, namely the width of the stream, its radial\noffset from the orbit, and the properties of the gap at the progenitor's\nlocation. Our results provide a step towards using stellar streams to constrain\nthe BH content of dissolved (globular) star clusters.", "author": null, "tags": [["Document Clustering", 0.22779000467612473]], "link": [{"@href": "http://arxiv.org/abs/2402.06393v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06393v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06391v1", "title": "Measures on effect algebras", "summary": "We study measures defined on effect algebras. We characterize real-valued\nmeasures on effect algebras and find a class of effect algebras, that include\nthe natural effect algebras of sets, on which sigma-additive measures with\nvalues in a finite dimensional Banach space are always bounded. We also prove\nthat in effect algebras the Nikodym and the Grothendieck properties together\nimply the Vitali-Hahn-Saks property, and find an example of an effect algebra\nverifying the Vitali-Hahn-Saks property but failing to have the Nikodym\nproperty. Finally, we define the concept of variation for vector measures on\neffect algebras proving that in effect algebras verifying the Riesz\nDecomposition Property, the variation of a finitely additive vector measure is\na finitely additive positive measure.", "author": null, "tags": [], "link": [{"@title": "doi", "@href": "http://dx.doi.org/10.1515/ms-2017-0211", "@rel": "related"}, {"@href": "http://arxiv.org/abs/2402.06391v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06391v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06389v1", "title": "Human Aesthetic Preference-Based Large Text-to-Image Model\n  Personalization: Kandinsky Generation as an Example", "summary": "With the advancement of neural generative capabilities, the art community has\nactively embraced GenAI (generative artificial intelligence) for creating\npainterly content. Large text-to-image models can quickly generate\naesthetically pleasing outcomes. However, the process can be non-deterministic\nand often involves tedious trial-and-error, as users struggle with formulating\neffective prompts to achieve their desired results. This paper introduces a\nprompting-free generative approach that empowers users to automatically\ngenerate personalized painterly content that incorporates their aesthetic\npreferences in a customized artistic style. This approach involves utilizing\n``semantic injection'' to customize an artist model in a specific artistic\nstyle, and further leveraging a genetic algorithm to optimize the prompt\ngeneration process through real-time iterative human feedback. By solely\nrelying on the user's aesthetic evaluation and preference for the artist\nmodel-generated images, this approach creates the user a personalized model\nthat encompasses their aesthetic preferences and the customized artistic style.", "author": null, "tags": [["Chatbot Development", 0.17402224473263664]], "link": [{"@href": "http://arxiv.org/abs/2402.06389v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06389v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06387v1", "title": "A Transversal Study of Fundamental Frequency Contours in Parkinsonian\n  Voices", "summary": "A transversal study of the pitch variability of parkinsonian voices in read\nspeech is presented. 30 patients suffering from Parkinson's disease (PD) and 32\nhealthy speakers were recorded while reading a text without voiceless phonemes.\nThe fundamental frequency contours were calculated from the recordings, and the\nfollowing measures were used for describing them: mean, minimum, maximum, and\nstandard deviation of the estimated fundamental frequencies. Results based on\nthese measures indicate that the influence of PD on some aspects of intonation\ncan be masked by the effects of aging, especially for male voices. However,\nsome parameters such as the relative fundamental frequency range exhibit lower\ncorrelations with age than with PD stage, as evaluated using the Hoehn and Yahr\nscale. These correlations between relative fundamental frequency range and PD\nstage reach moderate-to-high values in the case of women. Additionally, three\nparameters describing the form of the fundamental frequency modulation spectrum\nwere investigated for correlation with age and PD stage. The study of this\nmodulation spectrum provides some insight into the ability of the speakers to\nplan the intonation of full phrases. For both male and female populations,\nsignificant correlations were found between parameters obtained from the\nmodulation spectrum of fundamental frequency and the PD stage. Nevertheless,\nthe quantitative assessment of the performance of regression models built from\nthese modulation parameters and fundamental frequency range suggests that such\nmeasures are likely to be of limited value in the early diagnosis of PD due to\ninter-speaker variability.", "author": null, "tags": [], "link": [{"@title": "doi", "@href": "http://dx.doi.org/10.1016/j.bspc.2019.02.021", "@rel": "related"}, {"@href": "http://arxiv.org/abs/2402.06387v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06387v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06384v1", "title": "pSTL-Bench: A Micro-Benchmark Suite for Assessing Scalability of C++\n  Parallel STL Implementations", "summary": "Since the advent of parallel algorithms in the C++17 Standard Template\nLibrary (STL), the STL has become a viable framework for creating\nperformance-portable applications. Given multiple existing implementations of\nthe parallel algorithms, a systematic, quantitative performance comparison is\nessential for choosing the appropriate implementation for a particular hardware\nconfiguration.\n  In this work, we introduce a specialized set of micro-benchmarks to assess\nthe scalability of the parallel algorithms in the STL. By selecting different\nbackends, our micro-benchmarks can be used on multi-core systems and GPUs.\n  Using the suite, in a case study on AMD and Intel CPUs and NVIDIA GPUs, we\nwere able to identify substantial performance disparities among different\nimplementations, including GCC+TBB, GCC+HPX, Intel's compiler with TBB, or\nNVIDIA's compiler with OpenMP and CUDA.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06384v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06384v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06377v1", "title": "High-Precision Geosteering via Reinforcement Learning and Particle\n  Filters", "summary": "Geosteering, a key component of drilling operations, traditionally involves\nmanual interpretation of various data sources such as well-log data. This\nintroduces subjective biases and inconsistent procedures. Academic attempts to\nsolve geosteering decision optimization with greedy optimization and\nApproximate Dynamic Programming (ADP) showed promise but lacked adaptivity to\nrealistic diverse scenarios. Reinforcement learning (RL) offers a solution to\nthese challenges, facilitating optimal decision-making through reward-based\niterative learning. State estimation methods, e.g., particle filter (PF),\nprovide a complementary strategy for geosteering decision-making based on\nonline information. We integrate an RL-based geosteering with PF to address\nrealistic geosteering scenarios. Our framework deploys PF to process real-time\nwell-log data to estimate the location of the well relative to the\nstratigraphic layers, which then informs the RL-based decision-making process.\nWe compare our method's performance with that of using solely either RL or PF.\nOur findings indicate a synergy between RL and PF in yielding optimized\ngeosteering decisions.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06377v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06377v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06375v1", "title": "Configuration of the magnetosome chain: a natural magnetic\n  nanoarchitecture", "summary": "Magnetospirillum gryphiswaldense is a microorganism with the ability to\nbiomineralize magnetite nanoparticles, called magnetosomes, and arrange them\ninto a chain that behaves like a magnetic compass. Rather than straight lines,\nmagnetosome chains are slightly bent, as evidenced by electron cryotomography.\nOur experimental and theoretical results suggest that due to the competition\nbetween the magnetocrystalline and shape anisotropies, the effective magnetic\nmoment of individual magnetosomes is tilted out of the [111] crystallographic\neasy axis of magnetite. This tilt does not affect the direction of the chain\nnet magnetic moment, which remains along the [111] axis, but explains the\narrangement of magnetosomes in helical-like shaped chains. Indeed, we\ndemonstrate that the chain shape can be reproduced by considering an interplay\nbetween the magnetic dipolar interactions between magnetosomes, ruled by the\norientation of the magnetosome magnetic moment, and a lipid/protein-based\nmechanism, modeled as an elastic recovery force exerted on the magnetosomes.", "author": null, "tags": [], "link": [{"@title": "doi", "@href": "http://dx.doi.org/10.1039/c7nr08493e", "@rel": "related"}, {"@href": "http://arxiv.org/abs/2402.06375v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06375v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06374v1", "title": "Deconfinement transitions in three-dimensional compact lattice Abelian\n  Higgs models with multiple-charge scalar fields", "summary": "We investigate the nature of the deconfinement transitions in\nthree-dimensional lattice Abelian Higgs models, in which a complex scalar field\nof integer charge $Q\\ge 2$ is minimally coupled with a compact $U(1)$ gauge\nfield. Their phase diagram presents two phases separated by a transition line\nwhere static charges $q$, with $q<Q$, deconfine. We argue that these\ndeconfinement transitions belong to the same universality class as transitions\nin generic three-dimensional ${\\mathbb Z}_Q$ gauge models. In particular, they\nare Ising-like for $Q=2$, of first order for $Q=3$, and belong to the\nthree-dimensional gauge $XY$ universality class for $Q\\ge 4$. This general\nscenario is supported by numerical finite-size scaling analyses of the energy\ncumulants for $Q=2$, $Q=4$, and $Q=6$.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06374v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06374v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06373v1", "title": "A new edge betweenness measure using a game theoretical approach: an\n  application to hierarchical community detection", "summary": "In this paper we formally define the hierarchical clustering network problem\n(HCNP) as the problem to find a good hierarchical partition of a network. This\nnew problem focuses on the dynamic process of the clustering rather than on the\nfinal picture of the clustering process. To address it, we introduce a new\nierarchical clustering algorithm in networks, based on a new shortest path\nbetweenness measure. To calculate it, the communication between each pair of\nnodes is weighed by he importance of the nodes that establish this\ncommunication. The weights or importance associated to each pair of nodes are\ncalculated as the Shapley value of a game, named as the linear modularity game.\nThis new measure, (the node-game shortest path betweenness measure), is used to\nobtain a hierarchical partition of the network by eliminating the link with the\nhighest value. To evaluate the performance of our algorithm, we introduce\nseveral criteria that allow us to compare different dendrograms of a network\nfrom two point of view: modularity and homogeneity. Finally, we propose a\nfaster algorithm based on a simplification of the node-game shortest path\nbetweenness measure, whose order is quadratic on sparse networks. This fast\nversion is competitive from a computational point of view with other\nhierarchical fast algorithms, and, in general, it provides better results.", "author": null, "tags": [["Text Mining", 0.16182599848997398]], "link": [{"@title": "doi", "@href": "http://dx.doi.org/10.3390/math9212666", "@rel": "related"}, {"@href": "http://arxiv.org/abs/2402.06373v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06373v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06368v1", "title": "Adaptive Downlink Localization and User Tracking in Near-Field and\n  Far-Field: A Trade-Off Analysis", "summary": "This paper considers the problem of downlink localization and user equipments\n(UEs) tracking with an adaptive procedure for a range of distances. We provide\nthe base station (BS) with two signaling schemes and the UEs with two\nlocalization algorithms, assuming far-field (FF) and near-field (NF)\nconditions, respectively. The proposed schemes employ different beam-sweep\npatterns, where their compatibility depends on the UE range. Consequently, the\nFF-NF distinction transcends the traditional definition. Our proposed NF scheme\nrequires beam-focusing on specific spots and more transmissions are required to\nsweep the area. Instead, the FF scheme assumes distant UEs, and fewer beams are\nsufficient. We derive a low-complexity algorithm that exploits the FF channel\nmodel and highlight its practical benefits and the limitations. Also, we\npropose an iterative adaptive procedure, where the signaling scheme is depends\non the expected accuracy-complexity trade-off. Multiple iterations introduce a\ntracking application, where the formed trajectory dictates the validity of our\nassumptions. Moreover, the range from the BS, where the FF signaling scheme can\nbe successfully employed, is investigated. We show that the conventional\nFraunhofer distance is not sufficient for adaptive localization and tracking\nalgorithms in the mixed NF and FF environment.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06368v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06368v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06367v1", "title": "TEE4EHR: Transformer Event Encoder for Better Representation Learning in\n  Electronic Health Records", "summary": "Irregular sampling of time series in electronic health records (EHRs) is one\nof the main challenges for developing machine learning models. Additionally,\nthe pattern of missing data in certain clinical variables is not at random but\ndepends on the decisions of clinicians and the state of the patient. Point\nprocess is a mathematical framework for analyzing event sequence data that is\nconsistent with irregular sampling patterns. Our model, TEE4EHR, is a\ntransformer event encoder (TEE) with point process loss that encodes the\npattern of laboratory tests in EHRs. The utility of our TEE has been\ninvestigated in a variety of benchmark event sequence datasets. Additionally,\nwe conduct experiments on two real-world EHR databases to provide a more\ncomprehensive evaluation of our model. Firstly, in a self-supervised learning\napproach, the TEE is jointly learned with an existing attention-based deep\nneural network which gives superior performance in negative log-likelihood and\nfuture event prediction. Besides, we propose an algorithm for aggregating\nattention weights that can reveal the interaction between the events. Secondly,\nwe transfer and freeze the learned TEE to the downstream task for the outcome\nprediction, where it outperforms state-of-the-art models for handling\nirregularly sampled time series. Furthermore, our results demonstrate that our\napproach can improve representation learning in EHRs and can be useful for\nclinical prediction tasks.", "author": null, "tags": [["Sequence Labeling", 0.17261702656258307]], "link": [{"@href": "http://arxiv.org/abs/2402.06367v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06367v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06363v1", "title": "StruQ: Defending Against Prompt Injection with Structured Queries", "summary": "Recent advances in Large Language Models (LLMs) enable exciting\nLLM-integrated applications, which perform text-based tasks by utilizing their\nadvanced language understanding capabilities. However, as LLMs have improved,\nso have the attacks against them. Prompt injection attacks are an important\nthreat: they trick the model to deviate from the original application's\ninstructions and instead follow user directives. These attacks rely on the\nLLM's ability to follow instructions and inability to separate the prompts and\nuser data. We introduce structured queries, a general approach to tackle this\nproblem. Structured queries separate prompts and data into two channels. We\nimplement a system that supports structured queries. This system is made of (1)\na secure front-end that formats a prompt and user data into a special format,\nand (2) a specially trained LLM that can produce high-quality outputs from\nthese inputs. The LLM is trained using a novel fine-tuning strategy: we convert\na base (non-instruction-tuned) LLM to a structured instruction-tuned model that\nwill only follow instructions in the prompt portion of a query. To do so, we\naugment standard instruction tuning datasets with examples that also include\ninstructions in the data portion of the query, and fine-tune the model to\nignore these. Our system significantly improves resistance to prompt injection\nattacks, with little or no impact on utility. Our code is released at\nhttps://github.com/Sizhe-Chen/PromptInjectionDefense.", "author": null, "tags": [["Language Modeling", 0.170737963327868]], "link": [{"@href": "http://arxiv.org/abs/2402.06363v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06363v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06360v1", "title": "CoSearchAgent: A Lightweight Collaborative Search Agent with Large\n  Language Models", "summary": "Collaborative search supports multiple users working together to accomplish a\nspecific search task. Research has found that designing lightweight\ncollaborative search plugins within instant messaging platforms aligns better\nwith users' collaborative habits. However, due to the complexity of multi-user\ninteraction scenarios, it is challenging to implement a fully functioning\nlightweight collaborative search system. Therefore, previous studies on\nlightweight collaborative search had to rely on the Wizard of Oz paradigm. In\nrecent years, large language models (LLMs) have been demonstrated to interact\nnaturally with users and achieve complex information-seeking tasks through\nLLM-based agents. Hence, to better support the research in collaborative\nsearch, in this demo, we propose CoSearchAgent, a lightweight collaborative\nsearch agent powered by LLMs. CoSearchAgent is designed as a Slack plugin that\ncan support collaborative search during multi-party conversations on this\nplatform. Equipped with the capacity to understand the queries and context in\nmulti-user conversations and the ability to search the Web for relevant\ninformation via APIs, CoSearchAgent can respond to user queries with answers\ngrounded on the relevant search results. It can also ask clarifying questions\nwhen the information needs are unclear. The proposed CoSearchAgent is highly\nflexible and would be useful for supporting further research on collaborative\nsearch. The code and demo video are accessible.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06360v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06360v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06359v1", "title": "Modelling Human Values for AI Reasoning", "summary": "One of today's most significant societal challenges is building AI systems\nwhose behaviour, or the behaviour it enables within communities of interacting\nagents (human and artificial), aligns with human values. To address this\nchallenge, we detail a formal model of human values for their explicit\ncomputational representation. To our knowledge, this has not been attempted as\nyet, which is surprising given the growing volume of research integrating\nvalues within AI. Taking as our starting point the wealth of research\ninvestigating the nature of human values from social psychology over the last\nfew decades, we set out to provide such a formal model. We show how this model\ncan provide the foundational apparatus for AI-based reasoning over values, and\ndemonstrate its applicability in real-world use cases. We illustrate how our\nmodel captures the key ideas from social psychology research and propose a\nroadmap for future integrated, and interdisciplinary, research into human\nvalues in AI. The ability to automatically reason over values not only helps\naddress the value alignment problem but also facilitates the design of AI\nsystems that can support individuals and communities in making more informed,\nvalue-aligned decisions. More and more, individuals and organisations are\nmotivated to understand their values more explicitly and explore whether their\nbehaviours and attitudes properly reflect them. Our work on modelling human\nvalues will enable AI systems to be designed and deployed to meet this growing\nneed.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06359v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06359v1", "@rel": "related", "@type": "application/pdf"}]}, {"id": "http://arxiv.org/abs/2402.06351v1", "title": "SWITCH: An Exemplar for Evaluating Self-Adaptive ML-Enabled Systems", "summary": "Addressing runtime uncertainties in Machine Learning-Enabled Systems (MLS) is\ncrucial for maintaining Quality of Service (QoS). The Machine Learning Model\nBalancer is a concept that addresses these uncertainties by facilitating\ndynamic ML model switching, showing promise in improving QoS in MLS. Leveraging\nthis concept, this paper introduces SWITCH, an exemplar developed to enhance\nself-adaptive capabilities in such systems through dynamic model switching in\nruntime. SWITCH is designed as a comprehensive web service catering to a broad\nrange of ML scenarios, with its implementation demonstrated through an object\ndetection use case. SWITCH provides researchers with a flexible platform to\napply and evaluate their ML model switching strategies, aiming to enhance QoS\nin MLS. SWITCH features advanced input handling, real-time data processing, and\nlogging for adaptation metrics supplemented with an interactive real-time\ndashboard for enhancing system observability. This paper details SWITCH's\narchitecture, self-adaptation strategies through ML model switching, and its\nempirical validation through a case study, illustrating its potential to\nimprove QoS in MLS. By enabling a hands-on approach to explore adaptive\nbehaviors in ML systems, SWITCH contributes a valuable tool to the SEAMS\ncommunity for research into self-adaptive mechanisms for MLS and their\npractical applications.", "author": null, "tags": [], "link": [{"@href": "http://arxiv.org/abs/2402.06351v1", "@rel": "alternate", "@type": "text/html"}, {"@title": "pdf", "@href": "http://arxiv.org/pdf/2402.06351v1", "@rel": "related", "@type": "application/pdf"}]}]