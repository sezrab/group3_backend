{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wrappers.acl_wrapper as acl\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\samue\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\acl_anthology\\anthology.py:92: SchemaMismatchWarning: Data directory contains a different schema.rnc as this library; you might need to update the data or the acl-anthology-py library.\n",
      "  warnings.warn(SchemaMismatchWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing papers from 2010\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m start \u001b[38;5;241m=\u001b[39m datetime(\u001b[38;5;241m2010\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      3\u001b[0m end \u001b[38;5;241m=\u001b[39m datetime(\u001b[38;5;241m2018\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m papers \u001b[38;5;241m=\u001b[39m \u001b[43macl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetAclAnthologyData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\samue\\Documents\\GitHub\\group3_backend\\wrappers\\acl_wrapper.py:82\u001b[0m, in \u001b[0;36mgetAclAnthologyData\u001b[1;34m(start_date, end_date, CONFIDENCE_THRESH)\u001b[0m\n\u001b[0;32m     78\u001b[0m url \u001b[38;5;241m=\u001b[39m paper\u001b[38;5;241m.\u001b[39mweb_url\n\u001b[0;32m     80\u001b[0m authors \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mauthor\u001b[38;5;241m.\u001b[39mfirst\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mauthor\u001b[38;5;241m.\u001b[39mlast\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m author \u001b[38;5;129;01min\u001b[39;00m paper\u001b[38;5;241m.\u001b[39mauthors]\n\u001b[1;32m---> 82\u001b[0m tags \u001b[38;5;241m=\u001b[39m \u001b[43mtagger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtag_abstract\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mabstract\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthresh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCONFIDENCE_THRESH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtopic_classifier/data/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     84\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m article \u001b[38;5;241m=\u001b[39m Article(\n\u001b[0;32m     86\u001b[0m     title\u001b[38;5;241m=\u001b[39mtitle,\n\u001b[0;32m     87\u001b[0m     source\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mACL Anthology\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     92\u001b[0m     published\u001b[38;5;241m=\u001b[39mpublished_date,\n\u001b[0;32m     93\u001b[0m )\n\u001b[0;32m     94\u001b[0m articles\u001b[38;5;241m.\u001b[39mappend(article)\n",
      "File \u001b[1;32mc:\\Users\\samue\\Documents\\GitHub\\group3_backend\\topic_classifier\\tagger.py:24\u001b[0m, in \u001b[0;36mtag_abstract\u001b[1;34m(abstract, thresh, data_folder)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# TODO: Generate stop words on a per-supertopic (supertopic, eg NLP) basis. For each topic, get the set of the N most common words, and the intersection of these sets is the stop words for that topic.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# The above is just brainstorming\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# https://stackoverflow.com/a/49121636\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# This is better ^\u001b[39;00m\n\u001b[0;32m     22\u001b[0m tf \u001b[38;5;241m=\u001b[39m paper_processor\u001b[38;5;241m.\u001b[39mtf(abstract)\n\u001b[1;32m---> 24\u001b[0m tvs \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_topic_vector_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m topics \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m topic, tv \u001b[38;5;129;01min\u001b[39;00m tvs\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32mc:\\Users\\samue\\Documents\\GitHub\\group3_backend\\topic_classifier\\utils.py:36\u001b[0m, in \u001b[0;36mload_topic_vector_file\u001b[1;34m(data_dir)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_topic_vector_file\u001b[39m(data_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 36\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/topic_vectors.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtopic_vectors\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m topic_vectors\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# start in 2000\n",
    "start = datetime(2010, 1, 1)\n",
    "end = datetime(2018, 1, 1)\n",
    "papers = acl.getAclAnthologyData(start_date=start, end_date=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminative Syntactic Reranking for Statistical Machine Translation by Simon Carter, Christof Monz\n",
      "Fast Approximate String Matching with Suffix Arrays and A* Parsing by Philipp Koehn, Jean Senellart\n",
      "Combining Confidence Estimation and Reference-based Metrics for Segment-level MT Evaluation by Lucia Specia, Jesús Giménez\n",
      "The Impact of Arabic Morphological Segmentation on Broad-coverage English-to-Arabic Statistical Machine Translation by Hassan Al-Haj, Alon Lavie\n",
      "Arabic Dialect Handling in Hybrid Machine Translation by Hassan Sawaf\n",
      "Coupling Statistical Machine Translation with Rule-based Transfer and Generation by Arafat Ahsan, Prasanth Kolachina, Sudheer Kolachina, Dipti Misra, Rajeev Sangal\n",
      "Semantically-Informed Syntactic Machine Translation: A Tree-Grafting Approach by Kathryn Baker, Michael Bloodgood, Chris Callison-Burch, Bonnie Dorr, Nathaniel Filardo, Lori Levin, Scott Miller, Christine Piatko\n",
      "A Cocktail of Deep Syntactic Features for Hierarchical Machine Translation by Daniel Stein, Stephan Peitz, David Vilar, Hermann Ney\n",
      "Using TERp to Augment the System Combination for SMT by Jinhua Du, Andy Way\n",
      "f-align: An Open-Source Alignment Tool for LFG f-Structures by Anton Bryl, Josef van Genabith\n",
      "Improved Phrase-based SMT with Syntactic Reordering Patterns Learned from Lattice Scoring by Jie Jiang, Jinhua Du, Andy Way\n",
      "Transliterating From All Languages by Ann Irvine, Chris Callison-Burch, Alexandre Klementiev\n",
      "Using Sublexical Translations to Handle the OOV Problem in MT by Chung-chi Huang, Ho-ching Yen, Shih-ting Huang, Jason Chang\n",
      "MT-based Sentence Alignment for OCR-generated Parallel Texts by Rico Sennrich, Martin Volk\n",
      "Detecting Cross-lingual Semantic Similarity Using Parallel PropBanks by Shumin Wu, Jinho Choi, Martha Palmer\n",
      "Combining Multi-Domain Statistical Machine Translation Models using Automatic Classifiers by Pratyush Banerjee, Jinhua Du, Baoli Li, Sudip Naskar, Andy Way, Josef van Genabith\n",
      "Using Variable Decoding Weight for Language Model in Statistical Machine Translation by Behrang Mohit, Rebecca Hwa, Alon Lavie\n",
      "Refining Word Alignment with Discriminative Training by Nadi Tomeh, Alexandre Allauzen, François Yvon, Guillaume Wisniewski\n",
      "Maximizing TM Performance through Sub-Tree Alignment and SMT by Ventsislav Zhechev, Josef van Genabith\n",
      "Choosing the Right Evaluation for Machine Translation: an Examination of Annotator and Automatic Metric Performance on Human Judgment Tasks by Michael Denkowski, Alon Lavie\n",
      "Incremental Re-training for Post-editing SMT by Daniel Hardt, Jakob Elming\n",
      "A Source-side Decoding Sequence Model for Statistical Machine Translation by Minwei Feng, Arne Mauser, Hermann Ney\n",
      "Supertags as Source Language Context in Hierarchical Phrase-Based SMT by Rejwanul Haque, Sudip Naskar, Antal van den Bosch, Andy Way\n",
      "Translating Structured Documents by George Foster, Pierre Isabelle, Roland Kuhn\n",
      "Extending the Hierarchical Phrase Based Model with Maximum Entropy Based BTG by Zhongjun He, Yao Meng, Hao Yu\n",
      "Transferring Syntactic Relations of Subject-Verb-Object Pattern in Chinese-to-Korean SMT by Jin-Ji Li, Jungi Kim, Jong-Hyeok Lee\n",
      "Improving the Post-Editing Experience using Translation Recommendation: A User Study by Yifan He, Yanjun Ma, Johann Roturier, Andy Way, Josef van Genabith\n",
      "Accuracy-Based Scoring for Phrase-Based Statistical Machine Translation by Sergio Penkale, Yanjun May, Daniel Galron, Andy Way\n",
      "Improving Reordering in Statistical Machine Translation from Farsi by Evgeny Matusov, Selçuk Köprü\n",
      "Chinese Syntactic Reordering through Contrastive Analysis of Predicate-predicate Patterns in Chinese-to-Korean SMT by Jin-Ji Li, Jungi Kim, Jong-Hyeok Lee\n",
      "Machine Translation Using Overlapping Alignments and SampleRank by Benjamin Roth, Andrew McCallum, Marc Dymetman, Nicola Cancedda\n",
      "A Comparison of Various Types of Extended Lexicon Models for Statistical Machine Translation by Matthias Huck, Martin Ratajczak, Patrick Lehnen, Hermann Ney\n",
      "A Discriminative Lexicon Model for Complex Morphology by Minwoo Jeong, Kristina Toutanova, Hisami Suzuki, Chris Quirk\n",
      "Voting on N-grams for Machine Translation System Combination by Kenneth Heafield, Alon Lavie\n",
      "Improved Statistical Machine Translation with Hybrid Phrasal Paraphrases Derived from Monolingual Text and a Shallow Lexical Resource by Yuval Marton\n",
      "Statistical Machine Translation of English-Manipuri using Morpho-syntactic and Semantic Information by Thoudam Doren Singh, Savaji Bandyopadhyay\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m paper \u001b[38;5;129;01min\u001b[39;00m papers:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(paper)\n\u001b[1;32m----> 4\u001b[0m     sleep(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "for paper in papers:\n",
    "    print(paper)\n",
    "    sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
